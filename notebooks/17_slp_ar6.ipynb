{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c91b40e6",
   "metadata": {},
   "source": [
    "# Sea Level Rise AR6\n",
    "Notebook environment to migrate netcdf files to CF compliant zarr & CoG. \n",
    "Note, this is still quite a mess, TODO: clean up.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aef40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional; code formatter, installed as jupyter lab extension\n",
    "#%load_ext lab_black\n",
    "# Optional; code formatter, installed as jupyter notebook extension\n",
    "%load_ext nb_black"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3f67518",
   "metadata": {},
   "source": [
    "### Configure OS independent paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b089ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import numpy.ma as ma\n",
    "import rasterio\n",
    "import rioxarray as rio\n",
    "from datacube.utils.cog import write_cog\n",
    "\n",
    "# Make root directories importable by appending root to path\n",
    "cwd = pathlib.Path().resolve()\n",
    "sys.path.append(os.path.dirname(cwd))\n",
    "\n",
    "# Get root paths\n",
    "home = pathlib.Path().home()\n",
    "root = home.root\n",
    "tmp_dir = home.joinpath(\"data\", \"tmp\")\n",
    "\n",
    "# Import custom functionality\n",
    "from etl import p_drive\n",
    "from etl.CF_compliancy_checker import check_compliancy, save_compliancy\n",
    "\n",
    "# Define (local and) remote drives\n",
    "coclico_data_dir = p_drive.joinpath(\"11205479-coclico\", \"FASTTRACK_DATA\")\n",
    "\n",
    "# Workaround to the Windows OS (10) udunits error after installation of cfchecker: https://github.com/SciTools/iris/issues/404\n",
    "os.environ[\"UDUNITS2_XML_PATH\"] = str(\n",
    "    home.joinpath(  # change to the udunits2.xml file dir in your Python installation\n",
    "        r\"Anaconda3\\pkgs\\udunits2-2.2.28-h892ecd3_0\\Library\\share\\udunits\\udunits2.xml\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# use local or remote data dir\n",
    "use_local_data = False\n",
    "ds_dirname = \"17_AR6_SLP_IPCC\"\n",
    "\n",
    "if use_local_data: \n",
    "    ds_dir = tmp_dir.joinpath(ds_dirname)\n",
    "else: \n",
    "    ds_dir = coclico_data_dir.joinpath(ds_dirname)\n",
    "\n",
    "if not ds_dir.exists():\n",
    "    raise FileNotFoundError(\"Directory with data does not exist.\")\n",
    "\n",
    "# directory to export result (make if not exists)\n",
    "cog_dir = ds_dir.joinpath(\"cog\")\n",
    "cog_dirs = ds_dir.joinpath(\"cogs\") # for making all files CF compliant\n",
    "cog_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef15534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths & files (manual input)\n",
    "ds_ssp26_path = ds_dir.joinpath(f\"total_ssp126_medium_confidence_values.nc\")\n",
    "ds_ssp45_path = ds_dir.joinpath(f\"total_ssp245_medium_confidence_values.nc\")\n",
    "ds_ssp85_path = ds_dir.joinpath(f\"total_ssp585_medium_confidence_values.nc\")\n",
    "ds_out_file = \"slr_medium_confidence_values\"\n",
    "CF_dir = coclico_data_dir.joinpath(r\"CF\")  # directory to save output CF check files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d9d1359",
   "metadata": {},
   "source": [
    "### Check CF compliancy original NetCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open datasets\n",
    "ds_26ssp = xr.open_dataset(ds_ssp26_path)\n",
    "ds_45ssp = xr.open_dataset(ds_ssp45_path)\n",
    "ds_85ssp = xr.open_dataset(ds_ssp85_path)\n",
    "\n",
    "# check original dataset\n",
    "ds_45ssp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19468d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_ssp26_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a038bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=ds_ssp26_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_ssp45_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cadb3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=ds_ssp45_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_ssp85_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c310346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=ds_ssp85_path, working_dir=CF_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c557e90",
   "metadata": {},
   "source": [
    "### Make CF compliant alterations to the NetCDF files (dataset dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5594baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,9))\n",
    "# plt.scatter(ds['lon'][1030:], ds['lat'][1030:], s=1)\n",
    "# #plt.scatter(ds['longitude'][2000:2100], ds['latitude'][2000:2100], s=1, c='r')\n",
    "# # plt.xlim(-10,50)\n",
    "# # plt.ylim(25,75)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rework the datasets\n",
    "\n",
    "# construct lon, lat grid arrays\n",
    "lonl = list(dict.fromkeys(ds_26ssp.lon.values[1030:])) # 1030 is where rasterized output starts (before we have arbitrary points)\n",
    "latl = list(dict.fromkeys(ds_26ssp.lat.values[1030:])) # 1030 is where rasterized output starts (before we have arbitrary points)\n",
    "\n",
    "# reshape sea level change variable\n",
    "slc_26 = ds_26ssp[\"sea_level_change\"].values[:,:,1030:].reshape(len(ds_26ssp.quantiles.values), len(ds_26ssp.years.values), len(latl), len(lonl)) # reshaped values\n",
    "slc_45 = ds_45ssp[\"sea_level_change\"].values[:,:,1030:].reshape(len(ds_45ssp.quantiles.values), len(ds_45ssp.years.values), len(latl), len(lonl)) # reshaped values\n",
    "slc_85 = ds_85ssp[\"sea_level_change\"].values[:,:,1030:].reshape(len(ds_85ssp.quantiles.values), len(ds_85ssp.years.values), len(latl), len(lonl)) # reshaped values\n",
    "\n",
    "# re-order monotonically\n",
    "slc_26 = slc_26[:,:,:,np.argsort(lonl)]\n",
    "slc_26 = slc_26[:,:,np.argsort(latl),:]\n",
    "slc_45 = slc_45[:,:,:,np.argsort(lonl)]\n",
    "slc_45 = slc_45[:,:,np.argsort(latl),:]\n",
    "slc_85 = slc_85[:,:,:,np.argsort(lonl)]\n",
    "slc_85 = slc_85[:,:,np.argsort(latl),:]\n",
    "\n",
    "# remove items that will be replaced\n",
    "ds_26ssp = ds_26ssp.drop_vars({\"sea_level_change\", \"lat\", \"lon\", \"locations\"}) # make clean reworked dataset\n",
    "ds_45ssp = ds_45ssp.drop_vars({\"sea_level_change\", \"lat\", \"lon\", \"locations\"}) # make clean reworked dataset\n",
    "ds_85ssp = ds_85ssp.drop_vars({\"sea_level_change\", \"lat\", \"lon\", \"locations\"}) # make clean reworked dataset\n",
    "\n",
    "# substitute new items\n",
    "ds_26ssp = ds_26ssp.assign_coords({\"lat\": sorted(latl), \"lon\": sorted(lonl)}) # assign dimensions\n",
    "ds_26ssp = ds_26ssp.assign(slr=([\"quantiles\", \"years\", \"lat\", \"lon\"], slc_26)) # assign data variabel\n",
    "ds_45ssp = ds_45ssp.assign_coords({\"lat\": sorted(latl), \"lon\": sorted(lonl)}) # assign dimensions\n",
    "ds_45ssp = ds_45ssp.assign(slr=([\"quantiles\", \"years\", \"lat\", \"lon\"], slc_45)) # assign data variabel\n",
    "ds_85ssp = ds_85ssp.assign_coords({\"lat\": sorted(latl), \"lon\": sorted(lonl)}) # assign dimensions\n",
    "ds_85ssp = ds_85ssp.assign(slr=([\"quantiles\", \"years\", \"lat\", \"lon\"], slc_85)) # assign data variabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51642e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetCDF variable and dimension alterations\n",
    "\n",
    "# rename or swap dimension names, the latter in case the name already exists as coordinate\n",
    "ds_26ssp = ds_26ssp.rename_dims({\"years\": \"time\", \"quantiles\": \"ensemble\"})\n",
    "ds_45ssp = ds_45ssp.rename_dims({\"years\": \"time\", \"quantiles\": \"ensemble\"})\n",
    "ds_85ssp = ds_85ssp.rename_dims({\"years\": \"time\", \"quantiles\": \"ensemble\"})\n",
    "\n",
    "# rename variables, if necessary\n",
    "ds_26ssp = ds_26ssp.rename_vars({\"years\": \"time\", \"quantiles\": \"ensemble\"})\n",
    "ds_45ssp = ds_45ssp.rename_vars({\"years\": \"time\", \"quantiles\": \"ensemble\"})\n",
    "ds_85ssp = ds_85ssp.rename_vars({\"years\": \"time\", \"quantiles\": \"ensemble\"})\n",
    "\n",
    "# make quantiles percentages for percentiles\n",
    "ds_26ssp['ensemble'] = np.around(ds_26ssp['ensemble'].values*100, decimals=2)\n",
    "ds_45ssp['ensemble'] = np.around(ds_45ssp['ensemble'].values*100, decimals=2)\n",
    "ds_85ssp['ensemble'] = np.around(ds_85ssp['ensemble'].values*100, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [ds_26ssp, ds_45ssp, ds_85ssp]:\n",
    "    i[\"time\"].attrs[\"long_name\"] = \"time\"\n",
    "    i[\"time\"].attrs[\"units\"] = \"yr\"\n",
    "    i[\"ensemble\"].attrs[\"long_name\"] = \"ensemble\"\n",
    "    i[\"ensemble\"].attrs[\"units\"] = \"1\"\n",
    "    i[\"lat\"].attrs[\"long_name\"] = \"latitude\"\n",
    "    i[\"lat\"].attrs[\"standard_name\"] = \"latitude\"\n",
    "    i[\"lat\"].attrs[\"units\"] = \"degrees_north\"\n",
    "    i[\"lon\"].attrs[\"long_name\"] = \"longitude\"\n",
    "    i[\"lon\"].attrs[\"standard_name\"] = \"longitude\"\n",
    "    i[\"lon\"].attrs[\"units\"] = \"degrees_east\"\n",
    "    i[\"slr\"].attrs[\"long_name\"] = \"sea level rise\"\n",
    "    i[\"slr\"].attrs[\"units\"] = \"mm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c63076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat datasets along new dimension with index values and name derived from pandas index object, if necessary\n",
    "dataset = xr.concat([ds_26ssp, ds_45ssp, ds_85ssp], dim=\"nscenarios\")\n",
    "dataset = dataset.assign_coords(\n",
    "    scenarios=(\"nscenarios\", np.array([\"SSP1-26\", \"SSP2-45\", \"SSP5-85\"], dtype=\"S\"))\n",
    ")\n",
    "\n",
    "# dataset = xr.concat(\n",
    "#     [dataset_historical, dataset_45rcp, dataset_85rcp],\n",
    "#     pd.Index([\"historical\", \"rcp45\", \"rcp85\"], name=\"scenarios\"),\n",
    "# )\n",
    "\n",
    "# dataset[\"scenarios\"].values.astype(\"U\") # retrieve scenarios as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-order shape of the data variables\n",
    "ds_26ssp = ds_26ssp.transpose(\"time\", \"lat\", \"lon\", \"ensemble\")\n",
    "ds_45ssp = ds_45ssp.transpose(\"time\", \"lat\", \"lon\", \"ensemble\")\n",
    "ds_85ssp = ds_85ssp.transpose(\"time\", \"lat\", \"lon\", \"ensemble\")\n",
    "dataset = dataset.transpose(\"nscenarios\", \"time\", \"lat\", \"lon\", \"ensemble\")\n",
    "\n",
    "# add or change certain variable / coordinate attributes\n",
    "dataset_attributes = {\n",
    "    \"scenarios\": {\"long_name\": \"climate scenarios\"}\n",
    "}  # specify custom (CF convention) attributes\n",
    "\n",
    "# add / overwrite attributes\n",
    "for k, v in dataset_attributes.items():\n",
    "    try:\n",
    "        dataset[k].attrs = dataset_attributes[k]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# NetCDF attribute alterations by means of metadata template\n",
    "f_global = open(ds_dir.joinpath(\"metadata_AR6_slp.json\"))\n",
    "meta_global = json.load(f_global)\n",
    "ds_list = [ds_26ssp, ds_45ssp, ds_85ssp, dataset]\n",
    "\n",
    "for i in ds_list:\n",
    "    for attr_name, attr_val in meta_global.items():\n",
    "        if attr_name == 'PROVIDERS':\n",
    "            attr_val = json.dumps(attr_val)\n",
    "        i.attrs[attr_name] = attr_val\n",
    "\n",
    "    i.attrs['Conventions'] = \"CF-1.8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the xarray dataset, best practice is to have as many as possible bold dimensions (dimension == coordinate name).\n",
    "# in this way, the Front-End can access the variable directly without having to index the variable first\n",
    "\n",
    "dataset\n",
    "# dataset[\"nscenarios\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b51c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new .nc files\n",
    "ds_26ssp.to_netcdf(path=str(ds_ssp26_path).replace(\".nc\", \"_CF.nc\"))\n",
    "ds_45ssp.to_netcdf(path=str(ds_ssp45_path).replace(\".nc\", \"_CF.nc\"))\n",
    "ds_85ssp.to_netcdf(path=str(ds_ssp85_path).replace(\".nc\", \"_CF.nc\"))\n",
    "dataset.to_netcdf(path=ds_dir.joinpath(ds_out_file + \"_CF.nc\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4db3707",
   "metadata": {},
   "source": [
    "### Check CF compliancy altered NetCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088dca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=str(ds_ssp26_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=str(ds_ssp26_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95bd5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=str(ds_ssp45_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213576cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=str(ds_ssp45_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb58d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=str(ds_ssp85_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=str(ds_ssp85_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93338230",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_dir.joinpath(ds_out_file + \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b988887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=ds_dir.joinpath(ds_out_file + \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beec081e",
   "metadata": {},
   "source": [
    "### write data to Zarr files (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba75d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to zarr in write mode (to overwrite if exists)\n",
    "#dataset.to_zarr(ds_dir.joinpath(\"%s.zarr\" % ds_out_file), mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset\n",
    "#ds_26ssp = xr.open_dataset(r\"P:\\11205479-coclico\\FASTTRACK_DATA\\17_AR6_SLP_IPCC\\total_ssp126_medium_confidence_values_CF.nc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3763c979",
   "metadata": {},
   "source": [
    "### Write data to CoG (CF compliant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dfee8ee",
   "metadata": {},
   "source": [
    "#### single CoG test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check CoG for one set of params\n",
    "\n",
    "# hard-coded input params\n",
    "ENSEMBLE = 50.0 # select ensemble\n",
    "TIME = 0 # select timestep (indices)\n",
    "VARIABLE = \"slr\" # select variable\n",
    "SSP = 0 # select scenario (indices)\n",
    "\n",
    "# open the dataset\n",
    "ds_fp = ds_dir.joinpath(ds_out_file + \"_CF.nc\")\n",
    "ds = xr.open_dataset(ds_fp)\n",
    "\n",
    "# make array 2d and fix spatial dimensions and crs\n",
    "rds = ds.sel({\"ensemble\": ENSEMBLE, \"nscenarios\": SSP}).isel(time=TIME)[VARIABLE] \n",
    "\n",
    "rds.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\")\n",
    "if not rds.rio.crs:\n",
    "    rds = rds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "# convert to dataset\n",
    "rdsd = rds.to_dataset()\n",
    "\n",
    "# add all attributes (again)\n",
    "for attr_name, attr_val in meta_global.items():\n",
    "    if attr_name == 'PROVIDERS':\n",
    "        attr_val = json.dumps(attr_val)\n",
    "    if attr_name == \"MEDIA_TYPE\": # change media type to tiff, leave the rest as is\n",
    "        attr_val = \"IMAGE/TIFF\"\n",
    "    rdsd.attrs[attr_name] = attr_val\n",
    "\n",
    "rdsd.attrs['Conventions'] = \"CF-1.8\"\n",
    "\n",
    "# export file\n",
    "ssp_str = rdsd[\"scenarios\"].item().decode(\"utf-8\") # fix scenario string \n",
    "fname = f\"{VARIABLE}_{ssp_str}_ens{float(ENSEMBLE)}_time{TIME}_CF.GeoTiff\"\n",
    "outpath = cog_dir.joinpath(fname)\n",
    "rdsd.rio.to_raster(outpath, driver=\"GTiff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522787fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to nc for quick CF compliancy check..\n",
    "rdsd.to_netcdf(path=cog_dir.joinpath(fname.replace(\".GeoTiff\", \".nc\")))\n",
    "CF_dir = coclico_data_dir.joinpath(r\"CF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c649388",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=cog_dir.joinpath(fname.replace(\".GeoTiff\", \".nc\")), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=cog_dir.joinpath(fname.replace(\".GeoTiff\", \".nc\")), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c580844",
   "metadata": {},
   "source": [
    "##### Note, TIFFs are way less flexible in variables and therefore no CF compliancy check is needed. Data will always be an array with band, y, x as dimensions and band, y, x, spatial_ref as coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rio.open_rasterio(outpath, masked=True)\n",
    "data.plot()\n",
    "#rds.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c29375",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1fb3349",
   "metadata": {},
   "source": [
    "#### Multiple CoGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do for all CoGs (CF compliant)\n",
    "\n",
    "# open the dataset\n",
    "ds_fp = ds_dir.joinpath(ds_out_file + \"_CF.nc\")\n",
    "ds = xr.open_dataset(ds_fp)\n",
    "\n",
    "for idx, scen in enumerate(ds[\"scenarios\"].values):\n",
    "    ssp = scen.decode(\"utf-8\")\n",
    "\n",
    "    # format ssp name for filenaming\n",
    "    ssp_name = \"ssp=%s\"%ssp.strip(\"SSP\")\n",
    "    print(ssp_name)\n",
    "\n",
    "    # extract list of data variables\n",
    "    variables = set(ds.variables) - set(ds.dims) - set(ds.coords)\n",
    "    #print(variables)\n",
    "\n",
    "    ntimes = ds.dims[\"time\"]\n",
    "    for ntime in range(ntimes):\n",
    "        ds2 = ds.copy()\n",
    "        ds2 = ds2.isel({\"time\": ntime})\n",
    "\n",
    "        # extract time for use tif naming (dataset specific)\n",
    "        time_name = str(ds2.time.values)\n",
    "\n",
    "        for var_name in variables:\n",
    "            da = ds2.sel({\"nscenarios\": idx})[var_name]\n",
    "\n",
    "            for idv, ens in enumerate(da[\"ensemble\"].values):\n",
    "                da2 = da.isel({\"ensemble\": idv})\n",
    "\n",
    "                # add crs and spatial dims\n",
    "                da2.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\")\n",
    "                if not da2.rio.crs:\n",
    "                    da2 = da2.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "                # compose tif name\n",
    "                fname = time_name + \".tif\"\n",
    "                blob_name = pathlib.Path(ssp_name, var_name + \"_ens%s\"%np.around(ens, decimals=2), fname)\n",
    "                outpath = cog_dirs.joinpath(blob_name)\n",
    "\n",
    "                # convert to dataset and save as geotiff & nc to check the CF compliancy\n",
    "                # dads = da2.to_dataset()\n",
    "\n",
    "                # # add all attributes (again)\n",
    "                # for attr_name, attr_val in meta_global.items():\n",
    "                #     if attr_name == 'PROVIDERS':\n",
    "                #         attr_val = json.dumps(attr_val)\n",
    "                #     if attr_name == \"MEDIA_TYPE\": # change media type to tiff, leave the rest as is\n",
    "                #         attr_val = \"IMAGE/TIFF\"\n",
    "                #     dads.attrs[attr_name] = attr_val\n",
    "\n",
    "                # dads.attrs['Conventions'] = \"CF-1.8\"\n",
    "\n",
    "                # save to .nc & geotiff\n",
    "                # fname = f\"{var_name}_{ssp}_ens{np.around(ens, decimals=2)}_time{ntime}_CF.GeoTiff\"\n",
    "                # outpath = cog_dir.joinpath(fname)\n",
    "                # dads.rio.to_raster(outpath, driver=\"GTiff\")\n",
    "                # dads.to_netcdf(path=cog_dir.joinpath(fname.replace(\".GeoTiff\", \".nc\")))\n",
    "                # CF_dir = coclico_data_dir.joinpath(r\"CF\")\n",
    "                \n",
    "                # make parent dir if not exists\n",
    "                outpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # set overwrite is false because tifs should be unique\n",
    "                try:\n",
    "                    write_cog(da2, fname=outpath, overwrite=False)\n",
    "                except OSError as e:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2665c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture cap --no-stderr\n",
    "# # check original CF compliancy\n",
    "\n",
    "# check_compliancy(testfile=cog_dir.joinpath(fname.replace(\".GeoTiff\", \".nc\")), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "# save_compliancy(cap, testfile=cog_dir.joinpath(fname.replace(\".GeoTiff\", \".nc\")), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22439946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "d52b8dfbdab1c939c3c4b10b0d762f4c8139583e350f28e123ee37db8f80dd50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
