{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c91b40e6",
   "metadata": {},
   "source": [
    "# Sea Level Rise AR5\n",
    "Notebook environment to migrate netcdf files to CF compliant zarr & CoG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aef40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional; code formatter, installed as jupyter lab extension\n",
    "#%load_ext lab_black\n",
    "# Optional; code formatter, installed as jupyter notebook extension\n",
    "%load_ext nb_black"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3f67518",
   "metadata": {},
   "source": [
    "### Configure OS independent paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b089ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import numpy.ma as ma\n",
    "import rasterio\n",
    "import rioxarray as rio\n",
    "from datacube.utils.cog import write_cog\n",
    "#load_dotenv()\n",
    "\n",
    "# Import custom functionality\n",
    "from coclicodata.drive_config import p_drive\n",
    "from coclicodata.etl.cf_compliancy_checker import check_compliancy, save_compliancy\n",
    "\n",
    "# Define (local and) remote drives\n",
    "coclico_data_dir = p_drive.joinpath(\"11207608-coclico\", \"FASTTRACK_DATA\")\n",
    "\n",
    "# Workaround to the Windows OS (10) udunits error after installation of cfchecker: https://github.com/SciTools/iris/issues/404\n",
    "os.environ[\"UDUNITS2_XML_PATH\"] = str(\n",
    "    pathlib.Path().home().joinpath(  # change to the udunits2.xml file dir in your Python installation\n",
    "        r\"Anaconda3\\pkgs\\udunits2-2.2.28-h892ecd3_0\\Library\\share\\udunits\\udunits2.xml\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# use local or remote data dir\n",
    "use_local_data = False\n",
    "ds_dirname = \"18_AR5_SLP_IPCC\"\n",
    "\n",
    "if use_local_data: \n",
    "    ds_dir = tmp_dir.joinpath(ds_dirname)\n",
    "else: \n",
    "    ds_dir = coclico_data_dir.joinpath(ds_dirname)\n",
    "\n",
    "if not ds_dir.exists():\n",
    "    raise FileNotFoundError(\"Directory with data does not exist.\")\n",
    "\n",
    "# directory to export result (make if not exists)\n",
    "cog_dir = ds_dir.joinpath(\"cog\") # for checking CF compliancy\n",
    "cog_dirs = ds_dir.joinpath(\"cogs\") # for making all files CF compliant\n",
    "cog_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef15534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths & files (manual input)\n",
    "ds_dir = coclico_data_dir.joinpath(\"18_AR5_SLP_IPCC\")\n",
    "ds_rcp26_path = ds_dir.joinpath(\"total-ens-slr-26-5.nc\")\n",
    "ds_rcp45_path = ds_dir.joinpath(\"total-ens-slr-45-5.nc\")\n",
    "ds_rcp85_path = ds_dir.joinpath(\"total-ens-slr-85-5.nc\")\n",
    "ds_out_file = \"total-ens-slr\"\n",
    "CF_dir = coclico_data_dir.joinpath(r\"CF\")  # directory to save output CF check files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d9d1359",
   "metadata": {},
   "source": [
    "### Check CF compliancy original NetCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open datasets\n",
    "ds_26rcp = xr.open_dataset(ds_rcp26_path)\n",
    "ds_45rcp = xr.open_dataset(ds_rcp45_path)\n",
    "ds_85rcp = xr.open_dataset(ds_rcp85_path)\n",
    "\n",
    "# check original dataset\n",
    "ds_26rcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19468d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_rcp26_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a038bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=ds_rcp26_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_rcp45_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cadb3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=ds_rcp45_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_rcp85_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c310346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=ds_rcp85_path, working_dir=CF_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c557e90",
   "metadata": {},
   "source": [
    "### Make CF compliant alterations to the NetCDF files (dataset dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51642e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetCDF variable and dimension alterations\n",
    "\n",
    "# rename or swap dimension names, the latter in case the name already exists as coordinate\n",
    "ds_26rcp = ds_26rcp.rename_dims(\n",
    "    {\"ens\": \"nmodelname\", \"bnds\": \"nv\"}  # nv = number of vertices\n",
    ")\n",
    "ds_45rcp = ds_45rcp.rename_dims({\"ens\": \"nmodelname\", \"bnds\": \"nv\"})\n",
    "ds_85rcp = ds_85rcp.rename_dims({\"ens\": \"nmodelname\", \"bnds\": \"nv\"})\n",
    "\n",
    "# # rename variables, if necessary\n",
    "# ds_26rcp = ds_26rcp.rename_vars({\"modelname\": \"ensemble\"})\n",
    "# ds_45rcp = ds_45rcp.rename_vars({\"modelname\": \"ensemble\"})\n",
    "# ds_85rcp = ds_85rcp.rename_vars({\"modelname\": \"ensemble\"})\n",
    "\n",
    "# # set some data variables to coordinates to avoid duplication of dimensions in later stage\n",
    "ds_26rcp = ds_26rcp.set_coords([\"modelname\", \"time_bnds\"])\n",
    "ds_45rcp = ds_45rcp.set_coords([\"modelname\", \"time_bnds\"])\n",
    "ds_85rcp = ds_85rcp.set_coords([\"modelname\", \"time_bnds\"])\n",
    "\n",
    "# encoding settings\n",
    "ds_26rcp.time_bnds.encoding[\n",
    "    \"_FillValue\"\n",
    "] = None  # xarray sets _FillValue automatically to None for float types, prevent this when needed\n",
    "ds_45rcp.time_bnds.encoding[\n",
    "    \"_FillValue\"\n",
    "] = None  # xarray sets _FillValue automatically to None for float types, prevent this when needed\n",
    "ds_85rcp.time_bnds.encoding[\n",
    "    \"_FillValue\"\n",
    "] = None  # xarray sets _FillValue automatically to None for float types, prevent this when needed\n",
    "\n",
    "# construct equal dimensions in ensembles\n",
    "# note, has to be partly manual as nensemble has to be indexed by ensemble strings\n",
    "strip_ensembles = list(\n",
    "    set([s.strip() for s in ds_85rcp[\"modelname\"].astype(str).values]).difference(\n",
    "        [s.strip() for s in ds_26rcp[\"modelname\"].astype(str).values]\n",
    "    )\n",
    ")  # remove these at correct positions in the data from RCP45 & RCP85\n",
    "strip_ensembles_idx = [\n",
    "    idx\n",
    "    for idx, j in enumerate(\n",
    "        [s.strip() for s in ds_85rcp[\"modelname\"].astype(str).values]\n",
    "    )\n",
    "    if j in strip_ensembles\n",
    "]\n",
    "ds_45rcp = ds_45rcp.drop_sel(\n",
    "    nmodelname=strip_ensembles_idx\n",
    ")  # remove indexed ensembles from RCP45\n",
    "ds_85rcp = ds_85rcp.drop_sel(\n",
    "    nmodelname=strip_ensembles_idx\n",
    ")  # remove indexed ensembles from RCP85\n",
    "\n",
    "# rework ensemble lists to get rid of excessive spaces\n",
    "ds_26rcp[\"modelname\"] = np.array(\n",
    "    [s.strip() for s in ds_26rcp[\"modelname\"].astype(str).values], dtype=\"S\"\n",
    ")\n",
    "ds_26rcp = ds_26rcp.swap_dims({\"modelname\": \"nmodelname\"})\n",
    "ds_45rcp[\"modelname\"] = np.array(\n",
    "    [s.strip() for s in ds_45rcp[\"modelname\"].astype(str).values], dtype=\"S\"\n",
    ")\n",
    "ds_45rcp = ds_45rcp.swap_dims({\"modelname\": \"nmodelname\"})\n",
    "ds_85rcp[\"modelname\"] = np.array(\n",
    "    [s.strip() for s in ds_85rcp[\"modelname\"].astype(str).values], dtype=\"S\"\n",
    ")\n",
    "ds_85rcp = ds_85rcp.swap_dims({\"modelname\": \"nmodelname\"})\n",
    "ds_26rcp[\"modelname\"].attrs[\n",
    "    \"long_name\"\n",
    "] = \"Model names in the same order as in totslr_ens var\"\n",
    "ds_45rcp[\"modelname\"].attrs[\n",
    "    \"long_name\"\n",
    "] = \"Model names in the same order as in totslr_ens var\"\n",
    "ds_85rcp[\"modelname\"].attrs[\n",
    "    \"long_name\"\n",
    "] = \"Model names in the same order as in totslr_ens var\"\n",
    "\n",
    "# info on all attributes\n",
    "# !ncinfo -v totslr {ds_rcp26_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c63076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat datasets along new dimension with index values and name derived from pandas index object, if necessary\n",
    "dataset = xr.concat([ds_26rcp, ds_45rcp, ds_85rcp], dim=\"nscenarios\")\n",
    "dataset = dataset.assign_coords(\n",
    "    scenarios=(\"nscenarios\", np.array([\"RCP26\", \"RCP45\", \"RCP85\"], dtype=\"S\"))\n",
    ")\n",
    "\n",
    "# dataset = xr.concat(\n",
    "#     [dataset_historical, dataset_45rcp, dataset_85rcp],\n",
    "#     pd.Index([\"historical\", \"rcp45\", \"rcp85\"], name=\"scenarios\"),\n",
    "# )\n",
    "\n",
    "# dataset[\"scenarios\"].values.astype(\"U\") # retrieve scenarios as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-order shape of the data variables\n",
    "ds_26rcp = ds_26rcp.transpose(\"time\", \"lat\", \"lon\", \"nv\", \"nmodelname\")\n",
    "ds_45rcp = ds_45rcp.transpose(\"time\", \"lat\", \"lon\", \"nv\", \"nmodelname\")\n",
    "ds_85rcp = ds_85rcp.transpose(\"time\", \"lat\", \"lon\", \"nv\", \"nmodelname\")\n",
    "dataset = dataset.transpose(\"nscenarios\", \"time\", \"lat\", \"lon\", \"nv\", \"nmodelname\")\n",
    "\n",
    "# add or change certain variable / coordinate attributes\n",
    "dataset_attributes = {\n",
    "    \"scenarios\": {\"long_name\": \"climate scenarios\"},\n",
    "    \"modelname\": {\"long_name\": \"Model names in the same order as in totslr_ens var\"},\n",
    "}  # specify custom (CF convention) attributes\n",
    "\n",
    "# add / overwrite attributes\n",
    "for k, v in dataset_attributes.items():\n",
    "    try:\n",
    "        dataset[k].attrs = dataset_attributes[k]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af852f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# NetCDF attribute alterations by means of metadata template\n",
    "f_global = open(ds_dir.joinpath(\"metadata_AR5_slp.json\"))\n",
    "meta_global = json.load(f_global)\n",
    "\n",
    "ds_list = [ds_26rcp, ds_45rcp, ds_85rcp, dataset]\n",
    "for i in ds_list:\n",
    "    for attr_name, attr_val in meta_global.items():\n",
    "        if attr_name == 'PROVIDERS':\n",
    "            attr_val = json.dumps(attr_val)\n",
    "        i.attrs[attr_name] = attr_val\n",
    "\n",
    "    i.attrs['Conventions'] = \"CF-1.8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the model dimension, will not be used in the platform. Use the average of the 16 models ('totslr')\n",
    "ds_26rcp = ds_26rcp.drop_dims(\"nmodelname\")\n",
    "ds_45rcp = ds_45rcp.drop_dims(\"nmodelname\")\n",
    "ds_85rcp = ds_85rcp.drop_dims(\"nmodelname\")\n",
    "dataset = dataset.drop_dims(\"nmodelname\")\n",
    "\n",
    "# iterate over ds_list and replace variables by merging them together\n",
    "ds_list_new = []\n",
    "for i in [ds_26rcp, ds_45rcp, ds_85rcp, dataset]:\n",
    "\n",
    "    # merge loerr, totslr & hierr as ensembles (5%, 50% 95%) and assign new variable called 'slr'\n",
    "    i_arr = xr.concat([i[\"loerr\"], i[\"totslr\"], i[\"hierr\"]], pd.Index([5.0, 50.0, 95.0], name=\"ensemble\"))\n",
    "    i = i.drop_vars([\"loerr\", \"totslr\", \"hierr\"])\n",
    "    i = i.assign(slr = i_arr)\n",
    "\n",
    "    # add / replace metadata names\n",
    "    i[\"time_bnds\"].attrs[\"long_name\"] = \"time boundaries\"\n",
    "    i[\"lon\"].attrs[\"long_name\"] = \"longitude\"\n",
    "    i[\"lat\"].attrs[\"long_name\"] = \"latitude\"\n",
    "    i[\"slr\"].attrs[\"long_name\"] = \"sea level rise\"\n",
    "    i[\"ensemble\"].attrs[\"long_name\"] = \"ensemble\"\n",
    "    i[\"ensemble\"].attrs[\"units\"] = \"1\"\n",
    "    ds_list_new.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the xarray dataset, best practice is to have as many as possible bold dimensions (dimension == coordinate name).\n",
    "# in this way, the Front-End can access the variable directly without having to index the variable first\n",
    "\n",
    "# assign ds_list_new sets again to variables\n",
    "ds_26rcp = ds_list_new[0]\n",
    "ds_45rcp = ds_list_new[1]\n",
    "ds_85rcp = ds_list_new[2]\n",
    "dataset = ds_list_new[3]\n",
    "\n",
    "dataset\n",
    "# dataset[\"nscenarios\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b51c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new .nc files\n",
    "ds_26rcp.to_netcdf(path=str(ds_rcp26_path).replace(\".nc\", \"_CF.nc\"))\n",
    "ds_45rcp.to_netcdf(path=str(ds_rcp45_path).replace(\".nc\", \"_CF.nc\"))\n",
    "ds_85rcp.to_netcdf(path=str(ds_rcp85_path).replace(\".nc\", \"_CF.nc\"))\n",
    "dataset.to_netcdf(path=ds_dir.joinpath(ds_out_file + \"_CF.nc\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4db3707",
   "metadata": {},
   "source": [
    "### Check CF compliancy altered NetCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088dca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=str(ds_rcp26_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=str(ds_rcp26_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95bd5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=str(ds_rcp45_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213576cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=str(ds_rcp45_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb58d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=str(ds_rcp85_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=str(ds_rcp85_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93338230",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_dir.joinpath(ds_out_file + \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b988887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=ds_dir.joinpath(ds_out_file + \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beec081e",
   "metadata": {},
   "source": [
    "### write data to Zarr files (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba75d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to zarr in write mode (to overwrite if exists)\n",
    "#dataset.to_zarr(ds_dir.joinpath(\"%s.zarr\" % ds_out_file), mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset\n",
    "#ds_26rcp = xr.open_dataset(r\"p:\\11205479-coclico\\FASTTRACK_DATA\\18_AR5_SLP_IPCC\\total-ens-slr-26-5_CF.nc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24925bf6",
   "metadata": {},
   "source": [
    "### Write data to CoG (CF compliant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert time format\n",
    "import cftime\n",
    "def cftime_to_pdts(t: cftime._cftime) -> pd.Timestamp:\n",
    "    return pd.Timestamp(\n",
    "        t.year,\n",
    "        t.month,\n",
    "        t.day,\n",
    "        t.hour,\n",
    "        t.minute,\n",
    "        t.second,\n",
    "        t.microsecond,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40342a20",
   "metadata": {},
   "source": [
    "#### Single CoG test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check CoG for one set of params\n",
    "\n",
    "# hard-coded input params\n",
    "ENSEMBLE = 50.0 # select ensemble\n",
    "TIME = 0 # select timestep\n",
    "VARIABLE = \"slr\" # select variable\n",
    "RCP = 1 # select scenario\n",
    "\n",
    "# open the dataset\n",
    "ds_fp = ds_dir.joinpath(f\"total-ens-slr_CF.nc\")\n",
    "ds = xr.open_dataset(ds_fp)\n",
    "\n",
    "# make array 2d and fix time, spatial dimensions and crs\n",
    "rds = ds.isel(time=TIME) \n",
    "time_bounds = [\n",
    "    cftime_to_pdts(t).strftime(\"%Y-%m-%d\") for t in rds.time_bnds.values\n",
    "]\n",
    "time_name = \"_\".join([t for t in time_bounds])\n",
    "\n",
    "rds = rds.sel({\"ensemble\": ENSEMBLE, \"nscenarios\": RCP})[VARIABLE]\n",
    "rds.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\")\n",
    "if not rds.rio.crs:\n",
    "    rds = rds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "# reset some attributes\n",
    "rds[\"time\"] = np.array(cftime_to_pdts(rds[\"time\"].item()).strftime(\"%Y-%m-%d %H:%M:%S\"), dtype=\"S\")\n",
    "rds[\"time\"].attrs[\"standard_name\"] = \"time\"\n",
    "rds[\"time_bnds\"] = np.array(time_name, dtype=\"S\")\n",
    "rds[\"time_bnds\"].attrs[\"long_name\"] = \"time boundaries\"\n",
    "\n",
    "# convert to dataset\n",
    "rdsd = rds.to_dataset()\n",
    "\n",
    "# add all attributes (again)\n",
    "for attr_name, attr_val in meta_global.items():\n",
    "    if attr_name == 'PROVIDERS':\n",
    "        attr_val = json.dumps(attr_val)\n",
    "    if attr_name == \"MEDIA_TYPE\": # change media type to tiff, leave the rest as is\n",
    "        attr_val = \"IMAGE/TIFF\"\n",
    "    rdsd.attrs[attr_name] = attr_val\n",
    "\n",
    "rdsd.attrs['Conventions'] = \"CF-1.8\"\n",
    "\n",
    "# export file\n",
    "rcp_str = rdsd[\"scenarios\"].item().decode(\"utf-8\") # fix scenario string\n",
    "fname = f\"{VARIABLE}_{rcp_str}_ens{int(ENSEMBLE)}_time{TIME}_CF.GeoTiff\"\n",
    "outpath = cog_dir.joinpath(fname)\n",
    "rdsd.rio.to_raster(outpath, driver=\"GTiff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to nc for quick CF compliancy check..\n",
    "rdsd.to_netcdf(path=cog_dir.joinpath(fname.replace(\".GeoTiff\", \".nc\")))\n",
    "CF_dir = coclico_data_dir.joinpath(r\"CF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=cog_dir.joinpath(fname.replace(\".GeoTiff\", \".nc\")), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9bfac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=cog_dir.joinpath(fname.replace(\".GeoTiff\", \".nc\")), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb42a5f",
   "metadata": {},
   "source": [
    "##### Note, TIFFs are way less flexible in variables and therefore no CF compliancy check is needed. Data will always be an array with band, y, x as dimensions and band, y, x, spatial_ref as coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to check output\n",
    "data = rio.open_rasterio(outpath, masked=True)\n",
    "data.plot()\n",
    "#rds.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cbaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d78a39c",
   "metadata": {},
   "source": [
    "#### Multiple CoGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c96436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do for all CoGs (CF compliant)\n",
    "\n",
    "# open the dataset\n",
    "ds_fp = ds_dir.joinpath(f\"total-ens-slr_CF.nc\")\n",
    "ds = xr.open_dataset(ds_fp)\n",
    "\n",
    "#ds = make_cf_compliant(ds)\n",
    "\n",
    "# convert cf noleap yrs to datetime\n",
    "#ds[\"time\"] = ds.indexes[\"time\"].to_datetimeindex()\n",
    "\n",
    "for idx, scen in enumerate(ds[\"scenarios\"].values):\n",
    "    rcp = scen.decode(\"utf-8\")\n",
    "\n",
    "    # format rcp name for filenaming\n",
    "    rcp_name = \"rcp=%s\"%rcp.strip(\"RCP\")\n",
    "    print(rcp_name)\n",
    "\n",
    "    # extract list of data variables\n",
    "    variables = set(ds.variables) - set(ds.dims) - set(ds.coords)\n",
    "\n",
    "    #ds[\"modelname\"] = ds.coords[\"modelname\"].astype(str)\n",
    "\n",
    "    ntimes = ds.dims[\"time\"]\n",
    "    for ntime in range(ntimes):\n",
    "        ds2 = ds.copy()\n",
    "        ds2 = ds2.isel({\"time\": ntime})\n",
    "\n",
    "        # extract time boundaries for use in tif naming (dataset specific)\n",
    "        time_bounds = [\n",
    "            cftime_to_pdts(t).strftime(\"%Y-%m-%d\") for t in ds2.time_bnds.values\n",
    "        ]\n",
    "        time_name = \"_\".join([t for t in time_bounds])\n",
    "\n",
    "        for var_name in variables:\n",
    "            # time bounds are extracted, so nv dim can be dropped, as tiff should be 2D or 3D.\n",
    "            da = ds2.sel({\"nscenarios\": idx})[var_name]\n",
    "\n",
    "            for idv, ens in enumerate(da[\"ensemble\"].values):\n",
    "                da2 = da.isel({\"ensemble\": idv})\n",
    "\n",
    "                # add crs and spatial dims\n",
    "                da2.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\")\n",
    "                if not da2.rio.crs:\n",
    "                    da2 = da2.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "                # reset some variables and attributes (dataset specific)\n",
    "                da2[\"time\"] = np.array(cftime_to_pdts(da2[\"time\"].item()).strftime(\"%Y-%m-%d %H:%M:%S\"), dtype=\"S\")\n",
    "                da2[\"time\"].attrs[\"standard_name\"] = \"time\"\n",
    "                da2[\"time_bnds\"] = np.array(time_name, dtype=\"S\")\n",
    "                da2[\"time_bnds\"].attrs[\"long_name\"] = \"time boundaries\"\n",
    "\n",
    "                # compose tif name\n",
    "                fname = time_name + \".tif\"\n",
    "                blob_name = pathlib.Path(rcp_name, var_name + \"_ens%s\"%int(ens), fname)\n",
    "                outpath = cog_dirs.joinpath(blob_name)\n",
    "\n",
    "                # convert to dataset and save as geotiff & nc to check the CF compliancy\n",
    "                # dads = da2.to_dataset()\n",
    "\n",
    "                # # add all attributes (again)\n",
    "                # for attr_name, attr_val in meta_global.items():\n",
    "                #     if attr_name == 'PROVIDERS':\n",
    "                #         attr_val = json.dumps(attr_val)\n",
    "                #     if attr_name == \"MEDIA_TYPE\": # change media type to tiff, leave the rest as is\n",
    "                #         attr_val = \"IMAGE/TIFF\"\n",
    "                #     dads.attrs[attr_name] = attr_val\n",
    "\n",
    "                # dads.attrs['Conventions'] = \"CF-1.8\"\n",
    "\n",
    "                # save to .nc & geotiff\n",
    "                # fname = f\"{var_name}_{rcp}_ens{int(ens)}_time{ntime}_CF.GeoTiff\"\n",
    "                # outpath = cog_dir.joinpath(fname)\n",
    "                # dads.rio.to_raster(outpath, driver=\"GTiff\")\n",
    "                # dads.to_netcdf(path=cog_dir.joinpath(fname.replace(\".GeoTiff\", \".nc\")))\n",
    "                # CF_dir = coclico_data_dir.joinpath(r\"CF\")\n",
    "                \n",
    "                # make parent dir if not exists\n",
    "                outpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # set overwrite is false because tifs should be unique\n",
    "                try:\n",
    "                    write_cog(da2, fname=outpath, overwrite=False)\n",
    "                except OSError as e:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d67db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture cap --no-stderr\n",
    "# # check original CF compliancy\n",
    "\n",
    "# check_compliancy(testfile=cog_dir.joinpath(fname.replace(\".GeoTiff\", \".nc\")), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fbd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "# save_compliancy(cap, testfile=cog_dir.joinpath(fname.replace(\".GeoTiff\", \".nc\")), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa96b27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "d52b8dfbdab1c939c3c4b10b0d762f4c8139583e350f28e123ee37db8f80dd50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
