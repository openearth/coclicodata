{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coastal Mask\n",
    "\n",
    "Notebook environment to migrate TIF files to CF compliant CoG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 50;\n                var nbb_unformatted_code = \"# Optional; code formatter, installed as jupyter lab extension\\n#%load_ext lab_black\\n# Optional; code formatter, installed as jupyter notebook extension\\n%load_ext nb_black\";\n                var nbb_formatted_code = \"# Optional; code formatter, installed as jupyter lab extension\\n# %load_ext lab_black\\n# Optional; code formatter, installed as jupyter notebook extension\\n%load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optional; code formatter, installed as jupyter lab extension\n",
    "#%load_ext lab_black\n",
    "# Optional; code formatter, installed as jupyter notebook extension\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure OS independent paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soest\\AppData\\Local\\Temp\\ipykernel_15168\\783002922.py:7: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n",
      "c:\\Users\\soest\\AppData\\Local\\mambaforge\\envs\\coclico\\Lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:17: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 12.0.1. Please consider upgrading.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import standard packages\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from dotenv import load_dotenv\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "import rioxarray as rio\n",
    "#load_dotenv()\n",
    "\n",
    "# Import custom functionality\n",
    "from coclicodata.drive_config import p_drive\n",
    "from coclicodata.etl.cf_compliancy_checker import check_compliancy, save_compliancy\n",
    "from coastmonitor.io.utils import name_block\n",
    "\n",
    "# Define (local and) remote drives\n",
    "coclico_data_dir = p_drive.joinpath(\"11207608-coclico\", \"FULLTRACK_DATA\")\n",
    "\n",
    "# Workaround to the Windows OS (10) udunits error after installation of cfchecker: https://github.com/SciTools/iris/issues/404\n",
    "os.environ[\"UDUNITS2_XML_PATH\"] = str(\n",
    "    pathlib.Path().home().joinpath(  # change to the udunits2.xml file dir in your Python installation\n",
    "        r\"Anaconda3\\pkgs\\udunits2-2.2.28-h892ecd3_0\\Library\\share\\udunits\\udunits2.xml\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# use local or remote data dir\n",
    "use_local_data = False\n",
    "ds_dirname = \"WP4\"\n",
    "\n",
    "if use_local_data: \n",
    "    ds_dir = pathlib.Path().home().joinpath(\"data\", \"tmp\", ds_dirname)\n",
    "else: \n",
    "    ds_dir = coclico_data_dir.joinpath(ds_dirname)\n",
    "\n",
    "if not ds_dir.exists():\n",
    "    raise FileNotFoundError(\"Directory with data does not exist.\")\n",
    "\n",
    "# directory to export result (make if not exists)\n",
    "cog_dir = ds_dir.joinpath(\"cog\") # for checking CF compliancy\n",
    "cog_dirs = ds_dir.joinpath(\"cogs_final\") # for making all files CF compliant\n",
    "cog_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is already CF_compliant\n"
     ]
    }
   ],
   "source": [
    "print('Data is already CF_compliant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file structure for coastal flooding hazard maps\n",
    "\n",
    "import glob\n",
    "import rioxarray\n",
    "import rasterio\n",
    "from datacube.utils.cog import write_cog\n",
    "\n",
    "def generate_slices(num_chunks: int, chunk_size: int) -> Tuple[slice, slice]:\n",
    "    \"\"\"Generate slices for chunk-based iteration.\"\"\"\n",
    "    for i in range(num_chunks):\n",
    "        yield slice(i * chunk_size, (i + 1) * chunk_size)\n",
    "\n",
    "def get_paths(folder_structure, base_dir=''):\n",
    "    \"\"\"Generate paths for a folder structure defined by a dict\"\"\"\n",
    "    paths = []\n",
    "    for key, value in folder_structure.items():\n",
    "        if isinstance(value, dict):\n",
    "            paths.extend(get_paths(value, os.path.join(base_dir, key)))\n",
    "        elif isinstance(value, list):\n",
    "            if value:\n",
    "                for item in value:\n",
    "                    if item != \"\":\n",
    "                        paths.append(os.path.join(base_dir, key, item))\n",
    "            else:\n",
    "                paths.append(os.path.join(base_dir, key))\n",
    "        else:\n",
    "            continue\n",
    "    return paths\n",
    "\n",
    "# List different types on map folders\n",
    "map_types = [   'HIGH_DEFENDED_MAPS',\n",
    "                'LOW_DEFENDED_MAPS',\n",
    "                'UNDEFENDED_MAPS']\n",
    "\n",
    "# List all tif files present in first folder (note: it is assumed that the same files are present in all folders)\n",
    "tif_list = glob.glob(str(ds_dir.joinpath(\"data\", map_types[0],\"*.tif\")))\n",
    "\n",
    "# List the desired folder structure as a dict\n",
    "# NOTE: make sure the resulting path_list (based on folder structure) matches the tif_list\n",
    "folder_structure = {\n",
    "    \"Mean_spring_tide\": [],\n",
    "    \"RP\": [\"1000\", \"100\", \"1\"],\n",
    "    \"SLR\": {\n",
    "        \"High_end\": [\"2100\", \"2150\"],\n",
    "        \"SSP126\": [\"2100\"],\n",
    "        \"SSP245\": [\"2050\", \"2100\"],\n",
    "        \"SSP585\": [\"2030\", \"2050\", \"2100\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get list of paths for the folder structure\n",
    "path_list = get_paths(folder_structure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to open: Mean_spring_tide_HD.json\n",
      "trying to open: RP1000_HD.json\n",
      "trying to open: RP100_HD.json\n",
      "trying to open: RP1_HD.json\n",
      "trying to open: SLR_High-End_2100_subs_2050_HD.json\n",
      "trying to open: SLR_High-End_2150_subs_2050_HD.json\n",
      "trying to open: SLR_SSP126_2100_subs_2050_HD.json\n",
      "trying to open: SLR_SSP245_2050_subs_HD.json\n",
      "trying to open: SLR_SSP245_2100_subs_2050_HD.json\n",
      "trying to open: SLR_SSP585_2030_subs_HD.json\n",
      "trying to open: SLR_SSP585_2050_subs_HD.json\n",
      "trying to open: SLR_SSP585_2100_subs_2050_HD.json\n",
      "trying to open: Mean_spring_tide_LD.json\n",
      "trying to open: RP1000_LD.json\n",
      "trying to open: RP100_LD.json\n",
      "trying to open: RP1_LD.json\n",
      "trying to open: SLR_High-End_2100_subs_2050_LD.json\n",
      "trying to open: SLR_High-End_2150_subs_2050_LD.json\n",
      "trying to open: SLR_SSP126_2100_subs_2050_LD.json\n",
      "trying to open: SLR_SSP245_2050_subs_LD.json\n",
      "trying to open: SLR_SSP245_2100_subs_2050_LD.json\n",
      "trying to open: SLR_SSP585_2030_subs_LD.json\n",
      "trying to open: SLR_SSP585_2050_subs_LD.json\n",
      "trying to open: SLR_SSP585_2100_subs_2050_LD.json\n",
      "trying to open: High_tide.json\n",
      "trying to open: RP1000_UD.json\n",
      "trying to open: RP100_UD.json\n",
      "trying to open: RP1_UD.json\n",
      "trying to open: SLR_High-End_2100_subs_2050.json\n",
      "trying to open: SLR_High-End_2150_subs_2050.json\n",
      "trying to open: SLR_SSP126_2100_subs_2050.json\n",
      "trying to open: SLR_SSP245_2050_subs.json\n",
      "trying to open: SLR_SSP245_2100_subs_2050.json\n",
      "trying to open: SLR_SSP585_2030_subs.json\n",
      "trying to open: SLR_SSP585_2050_subs.json\n",
      "trying to open: SLR_SSP585_2100_subs_2050.json\n",
      "All .json files are working\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Not all meta_data.json files were correct and will break the loop. \n",
    "# CHECK if all meta_data \n",
    "\n",
    "for map_type in map_types:\n",
    "\n",
    "    # Get list of original tif's per map_type\n",
    "    tif_list = glob.glob(str(ds_dir.joinpath(\"data\", map_type,\"*.tif\")))\n",
    "    \n",
    "    for cur_path, cur_tif in zip(path_list, tif_list):\n",
    "        \n",
    "        print('trying to open: ' + str(os.path.basename(cur_tif.replace('tif','json'))))\n",
    "        \n",
    "        # Load meta data\n",
    "        cur_meta_data = open(os.path.join(os.path.dirname(cur_tif),'Metadata',os.path.basename(cur_tif.replace('tif','json'))))\n",
    "        cur_meta = json.load(cur_meta_data)\n",
    "\n",
    "        if map_type == map_types[-1] and cur_tif == tif_list[-1]:\n",
    "            print('All .json files are working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently working on: RP\\1 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\RP1_HD.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soest\\AppData\\Local\\Temp\\ipykernel_15168\\1179034474.py:27: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  num_x_chunks = math.ceil(fm_chunked.dims[\"x\"] / chunk_size)\n",
      "C:\\Users\\soest\\AppData\\Local\\Temp\\ipykernel_15168\\1179034474.py:28: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  num_y_chunks = math.ceil(fm_chunked.dims[\"y\"] / chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B01_epsg=3035_x=4023387_y=4596462.tif\n",
      "B01_epsg=3035_x=4023387_y=3777262.tif\n",
      "B01_epsg=3035_x=4023387_y=2958062.tif\n",
      "B01_epsg=3035_x=4023387_y=2138862.tif\n",
      "B01_epsg=3035_x=4023387_y=1425012.tif\n",
      "B01_epsg=3035_x=4842587_y=4596462.tif\n",
      "B01_epsg=3035_x=4842587_y=3777262.tif\n",
      "B01_epsg=3035_x=4842587_y=2958062.tif\n",
      "B01_epsg=3035_x=4842587_y=2138862.tif\n",
      "B01_epsg=3035_x=4842587_y=1425012.tif\n",
      "B01_epsg=3035_x=5661787_y=4596462.tif\n",
      "B01_epsg=3035_x=5661787_y=3777262.tif\n",
      "B01_epsg=3035_x=5661787_y=2958062.tif\n",
      "B01_epsg=3035_x=5661787_y=2138862.tif\n",
      "B01_epsg=3035_x=5661787_y=1425012.tif\n",
      "B01_epsg=3035_x=6480987_y=4596462.tif\n",
      "B01_epsg=3035_x=6480987_y=3777262.tif\n",
      "B01_epsg=3035_x=6480987_y=2958062.tif\n",
      "B01_epsg=3035_x=6480987_y=2138862.tif\n",
      "B01_epsg=3035_x=6480987_y=1425012.tif\n"
     ]
    }
   ],
   "source": [
    "# DO THE WORK\n",
    "\n",
    "# Iterate over the original tif files\n",
    "for map_type in [map_types[0]]:\n",
    "    \n",
    "    # Get list of original tif's per map_type\n",
    "    tif_list = glob.glob(str(ds_dir.joinpath(\"data\", map_type,\"*.tif\")))\n",
    "\n",
    "    for cur_path, cur_tif in zip([path_list[3]], [tif_list[3]]):\n",
    "\n",
    "        print('currently working on: '+str(cur_path)+' '+str(cur_tif))\n",
    "        \n",
    "        cur_dir = pathlib.Path(os.path.join(cog_dirs,map_type,cur_path))\n",
    "        cur_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "        fm = rioxarray.open_rasterio(\n",
    "            cur_tif, mask_and_scale=False\n",
    "        )  # .isel({\"x\":slice(0, 40000), \"y\":slice(0, 40000)})\n",
    "        fm = fm.assign_coords(band=(\"band\", [f\"B{k+1:02}\" for k in range(1)])) # NOTE: hard coded to 1, because one band\n",
    "        fm = fm.to_dataset(\"band\")\n",
    "\n",
    "        # chunk size \n",
    "        chunk_size = 2**15 # 16384, which is large, but OK for int8 datatype.\n",
    "\n",
    "        fm_chunked = fm.chunk({\"x\": chunk_size, \"y\": chunk_size})\n",
    "\n",
    "        num_x_chunks = math.ceil(fm_chunked.dims[\"x\"] / chunk_size)\n",
    "        num_y_chunks = math.ceil(fm_chunked.dims[\"y\"] / chunk_size)\n",
    "\n",
    "        # Load meta data\n",
    "        cur_meta_data = open(os.path.join(os.path.dirname(cur_tif),'Metadata',os.path.basename(cur_tif.replace('tif','json'))))\n",
    "        cur_meta = json.load(cur_meta_data)\n",
    "\n",
    "        for x_slice in generate_slices(num_x_chunks, chunk_size):\n",
    "            for y_slice in generate_slices(num_y_chunks, chunk_size):\n",
    "                chunk = fm_chunked.isel(x=x_slice, y=y_slice)\n",
    "\n",
    "                chunk = chunk.assign_coords(time=pd.Timestamp(2024, 3, 18).isoformat())\n",
    "\n",
    "                for var in chunk:\n",
    "\n",
    "                    da = chunk[var]\n",
    "                    \n",
    "                    da = (\n",
    "                        da.where(da != chunk.attrs['_FillValue'],-9999)\n",
    "                        .astype(\"float32\")\n",
    "                        .rio.write_nodata(-9999)\n",
    "                        .rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\")\n",
    "                    )\n",
    "\n",
    "                    item_name = name_block(\n",
    "                        da,\n",
    "                        storage_prefix=\"\",\n",
    "                        name_prefix=da.name,\n",
    "                        include_band=None, \n",
    "                        time_dim=False,\n",
    "                        x_dim=\"x\",\n",
    "                        y_dim=\"y\",\n",
    "                    )\n",
    "\n",
    "                    # hacky fix to get rid of the espg=None string\n",
    "                    if \"None\" in item_name:\n",
    "                        item_name = item_name.replace(\"None\", str(rasterio.crs.CRS(da.rio.crs).to_epsg()))\n",
    "\n",
    "                    print(item_name)\n",
    "\n",
    "                    # convert to dataset\n",
    "                    dad = da.to_dataset()\n",
    "\n",
    "                    # add all attributes (again)\n",
    "                    for attr_name, attr_val in cur_meta.items():\n",
    "                        if attr_name == 'PROVIDERS':\n",
    "                            attr_val = json.dumps(attr_val)\n",
    "                        if attr_name == \"MEDIA_TYPE\": # change media type to tiff, leave the rest as is\n",
    "                            attr_val = \"IMAGE/TIFF\"\n",
    "                        dad.attrs[attr_name] = attr_val\n",
    "\n",
    "                    dad.attrs['Conventions'] = \"CF-1.8\"\n",
    "\n",
    "                    # make parent dir if not exists\n",
    "\n",
    "                    outpath = cur_dir.joinpath(item_name)\n",
    "                    outpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                    # export file\n",
    "                    dad.rio.to_raster(outpath, compress=\"DEFLATE\", driver=\"COG\")\n",
    "\n",
    "                    # set overwrite is false because tifs should be unique\n",
    "                    # try:\n",
    "                    #     write_cog(da, fname=outpath, overwrite=False).compute()\n",
    "                    # except OSError as e:\n",
    "                    #     continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.attrs['_FillValue']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
