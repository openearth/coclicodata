{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coastal Mask\n",
    "\n",
    "Notebook environment to migrate TIF files to CF compliant CoG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"# Optional; code formatter, installed as jupyter lab extension\\n#%load_ext lab_black\\n# Optional; code formatter, installed as jupyter notebook extension\\n%load_ext nb_black\";\n                var nbb_formatted_code = \"# Optional; code formatter, installed as jupyter lab extension\\n# %load_ext lab_black\\n# Optional; code formatter, installed as jupyter notebook extension\\n%load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optional; code formatter, installed as jupyter lab extension\n",
    "#%load_ext lab_black\n",
    "# Optional; code formatter, installed as jupyter notebook extension\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure OS independent paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soest\\AppData\\Local\\Temp\\ipykernel_10496\\3021594099.py:6: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n",
      "c:\\Users\\soest\\AppData\\Local\\mambaforge\\envs\\coclico\\Lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:17: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 12.0.1. Please consider upgrading.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"# Import standard packages\\nimport os\\nimport pathlib\\nimport sys\\nimport numpy as np\\nimport geopandas as gpd\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport xarray as xr\\nfrom dotenv import load_dotenv\\nimport math\\nfrom pathlib import Path\\nfrom typing import Any, Dict, List, Optional, Tuple, Union\\nimport rioxarray as rio\\n#load_dotenv()\\n\\n# Import custom functionality\\nfrom coclicodata.drive_config import p_drive\\nfrom coclicodata.etl.cf_compliancy_checker import check_compliancy, save_compliancy\\nfrom coastmonitor.io.utils import name_block\\n\\n# Define (local and) remote drives\\ncoclico_data_dir = p_drive.joinpath(\\\"11207608-coclico\\\", \\\"FULLTRACK_DATA\\\")\\n\\n# Workaround to the Windows OS (10) udunits error after installation of cfchecker: https://github.com/SciTools/iris/issues/404\\nos.environ[\\\"UDUNITS2_XML_PATH\\\"] = str(\\n    pathlib.Path().home().joinpath(  # change to the udunits2.xml file dir in your Python installation\\n        r\\\"Anaconda3\\\\pkgs\\\\udunits2-2.2.28-h892ecd3_0\\\\Library\\\\share\\\\udunits\\\\udunits2.xml\\\"\\n    )\\n)\\n\\n# use local or remote data dir\\nuse_local_data = False\\nds_dirname = \\\"WP4\\\"\\n\\nif use_local_data: \\n    ds_dir = pathlib.Path().home().joinpath(\\\"data\\\", \\\"tmp\\\", ds_dirname)\\nelse: \\n    ds_dir = coclico_data_dir.joinpath(ds_dirname)\\n\\nif not ds_dir.exists():\\n    raise FileNotFoundError(\\\"Directory with data does not exist.\\\")\\n\\n# directory to export result (make if not exists)\\ncog_dir = ds_dir.joinpath(\\\"cog\\\") # for checking CF compliancy\\ncog_dirs = ds_dir.joinpath(\\\"cogs_final\\\") # for making all files CF compliant\\ncog_dir.mkdir(parents=True, exist_ok=True)\";\n                var nbb_formatted_code = \"# Import standard packages\\nimport os\\nimport pathlib\\nimport sys\\nimport numpy as np\\nimport geopandas as gpd\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport xarray as xr\\nfrom dotenv import load_dotenv\\nimport math\\nfrom pathlib import Path\\nfrom typing import Any, Dict, List, Optional, Tuple, Union\\nimport rioxarray as rio\\n\\n# load_dotenv()\\n\\n# Import custom functionality\\nfrom coclicodata.drive_config import p_drive\\nfrom coclicodata.etl.cf_compliancy_checker import check_compliancy, save_compliancy\\nfrom coastmonitor.io.utils import name_block\\n\\n# Define (local and) remote drives\\ncoclico_data_dir = p_drive.joinpath(\\\"11207608-coclico\\\", \\\"FULLTRACK_DATA\\\")\\n\\n# Workaround to the Windows OS (10) udunits error after installation of cfchecker: https://github.com/SciTools/iris/issues/404\\nos.environ[\\\"UDUNITS2_XML_PATH\\\"] = str(\\n    pathlib.Path()\\n    .home()\\n    .joinpath(  # change to the udunits2.xml file dir in your Python installation\\n        r\\\"Anaconda3\\\\pkgs\\\\udunits2-2.2.28-h892ecd3_0\\\\Library\\\\share\\\\udunits\\\\udunits2.xml\\\"\\n    )\\n)\\n\\n# use local or remote data dir\\nuse_local_data = False\\nds_dirname = \\\"WP4\\\"\\n\\nif use_local_data:\\n    ds_dir = pathlib.Path().home().joinpath(\\\"data\\\", \\\"tmp\\\", ds_dirname)\\nelse:\\n    ds_dir = coclico_data_dir.joinpath(ds_dirname)\\n\\nif not ds_dir.exists():\\n    raise FileNotFoundError(\\\"Directory with data does not exist.\\\")\\n\\n# directory to export result (make if not exists)\\ncog_dir = ds_dir.joinpath(\\\"cog\\\")  # for checking CF compliancy\\ncog_dirs = ds_dir.joinpath(\\\"cogs_final\\\")  # for making all files CF compliant\\ncog_dir.mkdir(parents=True, exist_ok=True)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import standard packages\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from dotenv import load_dotenv\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "import rioxarray as rio\n",
    "#load_dotenv()\n",
    "\n",
    "# Import custom functionality\n",
    "from coclicodata.drive_config import p_drive\n",
    "from coclicodata.etl.cf_compliancy_checker import check_compliancy, save_compliancy\n",
    "from coastmonitor.io.utils import name_block\n",
    "\n",
    "# Define (local and) remote drives\n",
    "coclico_data_dir = p_drive.joinpath(\"11207608-coclico\", \"FULLTRACK_DATA\")\n",
    "\n",
    "# Workaround to the Windows OS (10) udunits error after installation of cfchecker: https://github.com/SciTools/iris/issues/404\n",
    "os.environ[\"UDUNITS2_XML_PATH\"] = str(\n",
    "    pathlib.Path().home().joinpath(  # change to the udunits2.xml file dir in your Python installation\n",
    "        r\"Anaconda3\\pkgs\\udunits2-2.2.28-h892ecd3_0\\Library\\share\\udunits\\udunits2.xml\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# use local or remote data dir\n",
    "use_local_data = False\n",
    "ds_dirname = \"WP4\"\n",
    "\n",
    "if use_local_data: \n",
    "    ds_dir = pathlib.Path().home().joinpath(\"data\", \"tmp\", ds_dirname)\n",
    "else: \n",
    "    ds_dir = coclico_data_dir.joinpath(ds_dirname)\n",
    "\n",
    "if not ds_dir.exists():\n",
    "    raise FileNotFoundError(\"Directory with data does not exist.\")\n",
    "\n",
    "# directory to export result (make if not exists)\n",
    "cog_dir = ds_dir.joinpath(\"cog\") # for checking CF compliancy\n",
    "cog_dirs = ds_dir.joinpath(\"cogs_final\") # for making all files CF compliant\n",
    "cog_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 9;\n                var nbb_unformatted_code = \"# Project paths & files (manual input)\\ntest_path = ds_dir.joinpath(\\\"data\\\",\\\"TEST_MAPS\\\",\\\"RP100_LD.tif\\\")\\nCF_dir = ds_dir.joinpath(\\\"CF\\\")\";\n                var nbb_formatted_code = \"# Project paths & files (manual input)\\ntest_path = ds_dir.joinpath(\\\"data\\\", \\\"TEST_MAPS\\\", \\\"RP100_LD.tif\\\")\\nCF_dir = ds_dir.joinpath(\\\"CF\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Project paths & files (manual input)\n",
    "test_path = ds_dir.joinpath(\"data\",\"TEST_MAPS\",\"RP100_LD.tif\")\n",
    "CF_dir = ds_dir.joinpath(\"CF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"# NetCDF attribute alterations by means of metadata template\\nimport json\\n#meta_json = open(ds_dir.joinpath(\\\"data\\\",\\\"TEST_MAPS\\\",\\\"Metadata\\\", \\\"RP100_HD.json\\\"), 'r')\\n#meta_data = json.load(meta_json)\\n\\n## NOTE: original json file gives an error. At the end of the block for KEYWORDS and TAGS an extra comma and enter needed to be deleted. \";\n                var nbb_formatted_code = \"# NetCDF attribute alterations by means of metadata template\\nimport json\\n\\n# meta_json = open(ds_dir.joinpath(\\\"data\\\",\\\"TEST_MAPS\\\",\\\"Metadata\\\", \\\"RP100_HD.json\\\"), 'r')\\n# meta_data = json.load(meta_json)\\n\\n## NOTE: original json file gives an error. At the end of the block for KEYWORDS and TAGS an extra comma and enter needed to be deleted.\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NetCDF attribute alterations by means of metadata template\n",
    "import json\n",
    "#meta_json = open(ds_dir.joinpath(\"data\",\"TEST_MAPS\",\"Metadata\", \"RP100_HD.json\"), 'r')\n",
    "#meta_data = json.load(meta_json)\n",
    "\n",
    "## NOTE: original json file gives an error. At the end of the block for KEYWORDS and TAGS an extra comma and enter needed to be deleted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# export to nc for quick CF compliancy check..\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdad\u001b[49m\u001b[38;5;241m.\u001b[39mto_netcdf(path\u001b[38;5;241m=\u001b[39mcog_dir\u001b[38;5;241m.\u001b[39mjoinpath(item_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m      4\u001b[0m CF_dir\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dad' is not defined"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 11;\n                var nbb_unformatted_code = \"# export to nc for quick CF compliancy check..\\ndad.to_netcdf(path=cog_dir.joinpath(item_name.replace(\\\".tif\\\", \\\".nc\\\")))\\n\\nCF_dir\";\n                var nbb_formatted_code = \"# export to nc for quick CF compliancy check..\\ndad.to_netcdf(path=cog_dir.joinpath(item_name.replace(\\\".tif\\\", \\\".nc\\\")))\\n\\nCF_dir\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export to nc for quick CF compliancy check..\n",
    "dad.to_netcdf(path=cog_dir.joinpath(item_name.replace(\".tif\", \".nc\")))\n",
    "\n",
    "CF_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'item_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# check original CF compliancy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m check_compliancy(testfile\u001b[38;5;241m=\u001b[39mcog_dir\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[43mitem_name\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)), working_dir\u001b[38;5;241m=\u001b[39mCF_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'item_name' is not defined"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 12;\n                var nbb_unformatted_code = \"%%capture cap --no-stderr\\n# check original CF compliancy\\n\\ncheck_compliancy(testfile=cog_dir.joinpath(item_name.replace(\\\".tif\\\", \\\".nc\\\")), working_dir=CF_dir)\";\n                var nbb_formatted_code = \"%%capture cap --no-stderr\\n# check original CF compliancy\\n\\ncheck_compliancy(testfile=cog_dir.joinpath(item_name.replace(\\\".tif\\\", \\\".nc\\\")), working_dir=CF_dir)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=cog_dir.joinpath(item_name.replace(\".tif\", \".nc\")), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'item_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# save original CF compliancy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m save_compliancy(cap, testfile\u001b[38;5;241m=\u001b[39mcog_dir\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[43mitem_name\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)), working_dir\u001b[38;5;241m=\u001b[39mCF_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'item_name' is not defined"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 13;\n                var nbb_unformatted_code = \"# save original CF compliancy\\nsave_compliancy(cap, testfile=cog_dir.joinpath(item_name.replace(\\\".tif\\\", \\\".nc\\\")), working_dir=CF_dir)\";\n                var nbb_formatted_code = \"# save original CF compliancy\\nsave_compliancy(\\n    cap, testfile=cog_dir.joinpath(item_name.replace(\\\".tif\\\", \\\".nc\\\")), working_dir=CF_dir\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=cog_dir.joinpath(item_name.replace(\".tif\", \".nc\")), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test singel chunked tif, with data (not just nan's) to see if code is working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 114;\n                var nbb_unformatted_code = \"# Set up file structure for coastal flooding hazard maps\\n\\nimport glob\\nimport rioxarray\\nimport rasterio\\nfrom datacube.utils.cog import write_cog\\n\\ndef generate_slices(num_chunks: int, chunk_size: int) -> Tuple[slice, slice]:\\n    \\\"\\\"\\\"Generate slices for chunk-based iteration.\\\"\\\"\\\"\\n    for i in range(num_chunks):\\n        yield slice(i * chunk_size, (i + 1) * chunk_size)\\n\\ndef get_paths(folder_structure, base_dir=''):\\n    \\\"\\\"\\\"Generate paths for a folder structure defined by a dict\\\"\\\"\\\"\\n    paths = []\\n    for key, value in folder_structure.items():\\n        if isinstance(value, dict):\\n            paths.extend(get_paths(value, os.path.join(base_dir, key)))\\n        elif isinstance(value, list):\\n            if value:\\n                for item in value:\\n                    if item != \\\"\\\":\\n                        paths.append(os.path.join(base_dir, key, item))\\n            else:\\n                paths.append(os.path.join(base_dir, key))\\n        else:\\n            continue\\n    return paths\\n\\n# List different types on map folders\\nmap_types = [   'HIGH_DEFENDED_MAPS',\\n                'LOW_DEFENDED_MAPS',\\n                'UNDEFENDED_MAPS']\\n\\n# List all tif files present in first folder (note: it is assumed that the same files are present in all folders)\\ntif_list = glob.glob(str(ds_dir.joinpath(\\\"data\\\", map_types[0],\\\"*.tif\\\")))\\n\\n# List the desired folder structure as a dict\\n# NOTE: make sure the resulting path_list (based on folder structure) matches the tif_list\\nfolder_structure = {\\n    \\\"Mean_spring_tide\\\": [],\\n    \\\"RP\\\": [\\\"1000\\\", \\\"100\\\", \\\"1\\\"],\\n    \\\"SLR\\\": {\\n        \\\"High_end\\\": [\\\"2100\\\", \\\"2150\\\"],\\n        \\\"SSP126\\\": [\\\"2100\\\"],\\n        \\\"SSP245\\\": [\\\"2050\\\", \\\"2100\\\"],\\n        \\\"SSP585\\\": [\\\"2030\\\", \\\"2050\\\", \\\"2100\\\"]\\n    }\\n}\\n\\n# Get list of paths for the folder structure\\npath_list = get_paths(folder_structure)\\n\\n# Iterate over the original tif files\\n# for map_type in ['HIGH_DEFENDED_MAPS']:\\n#     for cur_path, cur_tif in zip([path_list[1]], [tif_list[1]]):\\n\\n#         print('currently working on: '+str(cur_path)+' '+str(cur_tif))\\n        \\n#         cur_dir = pathlib.Path(os.path.join(cog_dirs,map_type,cur_path))\\n#         cur_dir.mkdir(parents=True,exist_ok=True)\\n\\n#         fm = rioxarray.open_rasterio(\\n#             cur_tif, mask_and_scale=False\\n#         )  # .isel({\\\"x\\\":slice(0, 40000), \\\"y\\\":slice(0, 40000)})\\n#         fm = fm.assign_coords(band=(\\\"band\\\", [f\\\"B{k+1:02}\\\" for k in range(1)])) # NOTE: hard coded to 1, because one band\\n#         fm = fm.to_dataset(\\\"band\\\")\\n\\n#         # chunk size \\n#         chunk_size = 2**12 # 16384, which is large, but OK for int8 datatype.\\n\\n#         fm_chunked = fm.chunk({\\\"x\\\": chunk_size, \\\"y\\\": chunk_size})\\n\\n#         num_x_chunks = math.ceil(fm_chunked.dims[\\\"x\\\"] / chunk_size)\\n#         num_y_chunks = math.ceil(fm_chunked.dims[\\\"y\\\"] / chunk_size)\\n\\n#         # Load meta data\\n#         cur_meta_data = open(os.path.join(os.path.dirname(cur_tif),'Metadata',os.path.basename(cur_tif.replace('tif','json'))))\\n#         cur_meta = json.load(cur_meta_data)\\n\\n#         for x_slice in generate_slices(num_x_chunks, chunk_size):\\n#             for y_slice in generate_slices(num_y_chunks, chunk_size):\\n#                 chunk = fm_chunked.isel(x=x_slice, y=y_slice)\\n\\n#                 chunk = chunk.assign_coords(time=pd.Timestamp(2024, 3, 18).isoformat())\\n\\n#                 for var in chunk:\\n#                     print(x_slice, y_slice)\\n\\n#                     da = chunk[var]\\n\\n#                     da = (\\n#                         da.where(da != 3.3999999521443642e+38,-9999)\\n#                         .astype(\\\"float32\\\")\\n#                         .rio.write_nodata(-9999)\\n#                         .rio.set_spatial_dims(x_dim=\\\"x\\\", y_dim=\\\"y\\\")\\n#                     )\\n\\n#                     print(rasterio.crs.CRS(da.rio.crs).to_epsg())\\n\\n#                     item_name = name_block(\\n#                         da,\\n#                         storage_prefix=\\\"\\\",\\n#                         name_prefix=\\\"\\\",\\n#                         include_band=da.name,\\n#                         time_dim=False,\\n#                         x_dim=\\\"x\\\",\\n#                         y_dim=\\\"y\\\",\\n#                     )\\n\\n#                     # hacky fix to get rid of the espg=None string\\n#                     if \\\"None\\\" in item_name:\\n#                         item_name = item_name.replace(\\\"None\\\", str(rasterio.crs.CRS(da.rio.crs).to_epsg()))\\n\\n#                     if item_name == r'B01_x4210162.5_y3367662.5.tif':\\n\\n#                         # convert to dataset\\n#                         dad = da.to_dataset()\\n\\n#                         # add all attributes (again)\\n#                         for attr_name, attr_val in cur_meta.items():\\n#                             if attr_name == 'PROVIDERS':\\n#                                 attr_val = json.dumps(attr_val)\\n#                             if attr_name == \\\"MEDIA_TYPE\\\": # change media type to tiff, leave the rest as is\\n#                                 attr_val = \\\"IMAGE/TIFF\\\"\\n#                             dad.attrs[attr_name] = attr_val\\n\\n#                         dad.attrs['Conventions'] = \\\"CF-1.8\\\"\\n\\n#                         # make parent dir if not exists\\n#                         outpath = cur_dir.joinpath(item_name)\\n#                         outpath.parent.mkdir(parents=True, exist_ok=True)\\n\\n#                         # export file\\n#                         # dad.rio.to_raster(outpath, driver=\\\"COG\\\")\\n\\n#                         break\\n#                     if item_name == r'B01_x4210162.5_y3367662.5.tif': break\\n\\n#                 if item_name == r'B01_x4210162.5_y3367662.5.tif': break\\n\\n#             if item_name == r'B01_x4210162.5_y3367662.5.tif': break\\n\\n#         if item_name == r'B01_x4210162.5_y3367662.5.tif': break\\n\\n#     if item_name == r'B01_x4210162.5_y3367662.5.tif': break\\n                    # set overwrite is false because tifs should be unique\\n                    # try:\\n                    #     write_cog(da, fname=outpath, overwrite=False).compute()\\n                    # except OSError as e:\\n                    #     continue\";\n                var nbb_formatted_code = \"# Set up file structure for coastal flooding hazard maps\\n\\nimport glob\\nimport rioxarray\\nimport rasterio\\nfrom datacube.utils.cog import write_cog\\n\\n\\ndef generate_slices(num_chunks: int, chunk_size: int) -> Tuple[slice, slice]:\\n    \\\"\\\"\\\"Generate slices for chunk-based iteration.\\\"\\\"\\\"\\n    for i in range(num_chunks):\\n        yield slice(i * chunk_size, (i + 1) * chunk_size)\\n\\n\\ndef get_paths(folder_structure, base_dir=\\\"\\\"):\\n    \\\"\\\"\\\"Generate paths for a folder structure defined by a dict\\\"\\\"\\\"\\n    paths = []\\n    for key, value in folder_structure.items():\\n        if isinstance(value, dict):\\n            paths.extend(get_paths(value, os.path.join(base_dir, key)))\\n        elif isinstance(value, list):\\n            if value:\\n                for item in value:\\n                    if item != \\\"\\\":\\n                        paths.append(os.path.join(base_dir, key, item))\\n            else:\\n                paths.append(os.path.join(base_dir, key))\\n        else:\\n            continue\\n    return paths\\n\\n\\n# List different types on map folders\\nmap_types = [\\\"HIGH_DEFENDED_MAPS\\\", \\\"LOW_DEFENDED_MAPS\\\", \\\"UNDEFENDED_MAPS\\\"]\\n\\n# List all tif files present in first folder (note: it is assumed that the same files are present in all folders)\\ntif_list = glob.glob(str(ds_dir.joinpath(\\\"data\\\", map_types[0], \\\"*.tif\\\")))\\n\\n# List the desired folder structure as a dict\\n# NOTE: make sure the resulting path_list (based on folder structure) matches the tif_list\\nfolder_structure = {\\n    \\\"Mean_spring_tide\\\": [],\\n    \\\"RP\\\": [\\\"1000\\\", \\\"100\\\", \\\"1\\\"],\\n    \\\"SLR\\\": {\\n        \\\"High_end\\\": [\\\"2100\\\", \\\"2150\\\"],\\n        \\\"SSP126\\\": [\\\"2100\\\"],\\n        \\\"SSP245\\\": [\\\"2050\\\", \\\"2100\\\"],\\n        \\\"SSP585\\\": [\\\"2030\\\", \\\"2050\\\", \\\"2100\\\"],\\n    },\\n}\\n\\n# Get list of paths for the folder structure\\npath_list = get_paths(folder_structure)\\n\\n# Iterate over the original tif files\\n# for map_type in ['HIGH_DEFENDED_MAPS']:\\n#     for cur_path, cur_tif in zip([path_list[1]], [tif_list[1]]):\\n\\n#         print('currently working on: '+str(cur_path)+' '+str(cur_tif))\\n\\n#         cur_dir = pathlib.Path(os.path.join(cog_dirs,map_type,cur_path))\\n#         cur_dir.mkdir(parents=True,exist_ok=True)\\n\\n#         fm = rioxarray.open_rasterio(\\n#             cur_tif, mask_and_scale=False\\n#         )  # .isel({\\\"x\\\":slice(0, 40000), \\\"y\\\":slice(0, 40000)})\\n#         fm = fm.assign_coords(band=(\\\"band\\\", [f\\\"B{k+1:02}\\\" for k in range(1)])) # NOTE: hard coded to 1, because one band\\n#         fm = fm.to_dataset(\\\"band\\\")\\n\\n#         # chunk size\\n#         chunk_size = 2**12 # 16384, which is large, but OK for int8 datatype.\\n\\n#         fm_chunked = fm.chunk({\\\"x\\\": chunk_size, \\\"y\\\": chunk_size})\\n\\n#         num_x_chunks = math.ceil(fm_chunked.dims[\\\"x\\\"] / chunk_size)\\n#         num_y_chunks = math.ceil(fm_chunked.dims[\\\"y\\\"] / chunk_size)\\n\\n#         # Load meta data\\n#         cur_meta_data = open(os.path.join(os.path.dirname(cur_tif),'Metadata',os.path.basename(cur_tif.replace('tif','json'))))\\n#         cur_meta = json.load(cur_meta_data)\\n\\n#         for x_slice in generate_slices(num_x_chunks, chunk_size):\\n#             for y_slice in generate_slices(num_y_chunks, chunk_size):\\n#                 chunk = fm_chunked.isel(x=x_slice, y=y_slice)\\n\\n#                 chunk = chunk.assign_coords(time=pd.Timestamp(2024, 3, 18).isoformat())\\n\\n#                 for var in chunk:\\n#                     print(x_slice, y_slice)\\n\\n#                     da = chunk[var]\\n\\n#                     da = (\\n#                         da.where(da != 3.3999999521443642e+38,-9999)\\n#                         .astype(\\\"float32\\\")\\n#                         .rio.write_nodata(-9999)\\n#                         .rio.set_spatial_dims(x_dim=\\\"x\\\", y_dim=\\\"y\\\")\\n#                     )\\n\\n#                     print(rasterio.crs.CRS(da.rio.crs).to_epsg())\\n\\n#                     item_name = name_block(\\n#                         da,\\n#                         storage_prefix=\\\"\\\",\\n#                         name_prefix=\\\"\\\",\\n#                         include_band=da.name,\\n#                         time_dim=False,\\n#                         x_dim=\\\"x\\\",\\n#                         y_dim=\\\"y\\\",\\n#                     )\\n\\n#                     # hacky fix to get rid of the espg=None string\\n#                     if \\\"None\\\" in item_name:\\n#                         item_name = item_name.replace(\\\"None\\\", str(rasterio.crs.CRS(da.rio.crs).to_epsg()))\\n\\n#                     if item_name == r'B01_x4210162.5_y3367662.5.tif':\\n\\n#                         # convert to dataset\\n#                         dad = da.to_dataset()\\n\\n#                         # add all attributes (again)\\n#                         for attr_name, attr_val in cur_meta.items():\\n#                             if attr_name == 'PROVIDERS':\\n#                                 attr_val = json.dumps(attr_val)\\n#                             if attr_name == \\\"MEDIA_TYPE\\\": # change media type to tiff, leave the rest as is\\n#                                 attr_val = \\\"IMAGE/TIFF\\\"\\n#                             dad.attrs[attr_name] = attr_val\\n\\n#                         dad.attrs['Conventions'] = \\\"CF-1.8\\\"\\n\\n#                         # make parent dir if not exists\\n#                         outpath = cur_dir.joinpath(item_name)\\n#                         outpath.parent.mkdir(parents=True, exist_ok=True)\\n\\n#                         # export file\\n#                         # dad.rio.to_raster(outpath, driver=\\\"COG\\\")\\n\\n#                         break\\n#                     if item_name == r'B01_x4210162.5_y3367662.5.tif': break\\n\\n#                 if item_name == r'B01_x4210162.5_y3367662.5.tif': break\\n\\n#             if item_name == r'B01_x4210162.5_y3367662.5.tif': break\\n\\n#         if item_name == r'B01_x4210162.5_y3367662.5.tif': break\\n\\n#     if item_name == r'B01_x4210162.5_y3367662.5.tif': break\\n# set overwrite is false because tifs should be unique\\n# try:\\n#     write_cog(da, fname=outpath, overwrite=False).compute()\\n# except OSError as e:\\n#     continue\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up file structure for coastal flooding hazard maps\n",
    "\n",
    "import glob\n",
    "import rioxarray\n",
    "import rasterio\n",
    "from datacube.utils.cog import write_cog\n",
    "\n",
    "def generate_slices(num_chunks: int, chunk_size: int) -> Tuple[slice, slice]:\n",
    "    \"\"\"Generate slices for chunk-based iteration.\"\"\"\n",
    "    for i in range(num_chunks):\n",
    "        yield slice(i * chunk_size, (i + 1) * chunk_size)\n",
    "\n",
    "def get_paths(folder_structure, base_dir=''):\n",
    "    \"\"\"Generate paths for a folder structure defined by a dict\"\"\"\n",
    "    paths = []\n",
    "    for key, value in folder_structure.items():\n",
    "        if isinstance(value, dict):\n",
    "            paths.extend(get_paths(value, os.path.join(base_dir, key)))\n",
    "        elif isinstance(value, list):\n",
    "            if value:\n",
    "                for item in value:\n",
    "                    if item != \"\":\n",
    "                        paths.append(os.path.join(base_dir, key, item))\n",
    "            else:\n",
    "                paths.append(os.path.join(base_dir, key))\n",
    "        else:\n",
    "            continue\n",
    "    return paths\n",
    "\n",
    "# List different types on map folders\n",
    "map_types = [   'HIGH_DEFENDED_MAPS',\n",
    "                'LOW_DEFENDED_MAPS',\n",
    "                'UNDEFENDED_MAPS']\n",
    "\n",
    "# List all tif files present in first folder (note: it is assumed that the same files are present in all folders)\n",
    "tif_list = glob.glob(str(ds_dir.joinpath(\"data\", map_types[0],\"*.tif\")))\n",
    "\n",
    "# List the desired folder structure as a dict\n",
    "# NOTE: make sure the resulting path_list (based on folder structure) matches the tif_list\n",
    "folder_structure = {\n",
    "    \"Mean_spring_tide\": [],\n",
    "    \"RP\": [\"1000\", \"100\", \"1\"],\n",
    "    \"SLR\": {\n",
    "        \"High_end\": [\"2100\", \"2150\"],\n",
    "        \"SSP126\": [\"2100\"],\n",
    "        \"SSP245\": [\"2050\", \"2100\"],\n",
    "        \"SSP585\": [\"2030\", \"2050\", \"2100\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get list of paths for the folder structure\n",
    "path_list = get_paths(folder_structure)\n",
    "\n",
    "# Iterate over the original tif files\n",
    "# for map_type in ['HIGH_DEFENDED_MAPS']:\n",
    "#     for cur_path, cur_tif in zip([path_list[1]], [tif_list[1]]):\n",
    "\n",
    "#         print('currently working on: '+str(cur_path)+' '+str(cur_tif))\n",
    "        \n",
    "#         cur_dir = pathlib.Path(os.path.join(cog_dirs,map_type,cur_path))\n",
    "#         cur_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "#         fm = rioxarray.open_rasterio(\n",
    "#             cur_tif, mask_and_scale=False\n",
    "#         )  # .isel({\"x\":slice(0, 40000), \"y\":slice(0, 40000)})\n",
    "#         fm = fm.assign_coords(band=(\"band\", [f\"B{k+1:02}\" for k in range(1)])) # NOTE: hard coded to 1, because one band\n",
    "#         fm = fm.to_dataset(\"band\")\n",
    "\n",
    "#         # chunk size \n",
    "#         chunk_size = 2**12 # 16384, which is large, but OK for int8 datatype.\n",
    "\n",
    "#         fm_chunked = fm.chunk({\"x\": chunk_size, \"y\": chunk_size})\n",
    "\n",
    "#         num_x_chunks = math.ceil(fm_chunked.dims[\"x\"] / chunk_size)\n",
    "#         num_y_chunks = math.ceil(fm_chunked.dims[\"y\"] / chunk_size)\n",
    "\n",
    "#         # Load meta data\n",
    "#         cur_meta_data = open(os.path.join(os.path.dirname(cur_tif),'Metadata',os.path.basename(cur_tif.replace('tif','json'))))\n",
    "#         cur_meta = json.load(cur_meta_data)\n",
    "\n",
    "#         for x_slice in generate_slices(num_x_chunks, chunk_size):\n",
    "#             for y_slice in generate_slices(num_y_chunks, chunk_size):\n",
    "#                 chunk = fm_chunked.isel(x=x_slice, y=y_slice)\n",
    "\n",
    "#                 chunk = chunk.assign_coords(time=pd.Timestamp(2024, 3, 18).isoformat())\n",
    "\n",
    "#                 for var in chunk:\n",
    "#                     print(x_slice, y_slice)\n",
    "\n",
    "#                     da = chunk[var]\n",
    "\n",
    "#                     da = (\n",
    "#                         da.where(da != 3.3999999521443642e+38,-9999)\n",
    "#                         .astype(\"float32\")\n",
    "#                         .rio.write_nodata(-9999)\n",
    "#                         .rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\")\n",
    "#                     )\n",
    "\n",
    "#                     print(rasterio.crs.CRS(da.rio.crs).to_epsg())\n",
    "\n",
    "#                     item_name = name_block(\n",
    "#                         da,\n",
    "#                         storage_prefix=\"\",\n",
    "#                         name_prefix=\"\",\n",
    "#                         include_band=da.name,\n",
    "#                         time_dim=False,\n",
    "#                         x_dim=\"x\",\n",
    "#                         y_dim=\"y\",\n",
    "#                     )\n",
    "\n",
    "#                     # hacky fix to get rid of the espg=None string\n",
    "#                     if \"None\" in item_name:\n",
    "#                         item_name = item_name.replace(\"None\", str(rasterio.crs.CRS(da.rio.crs).to_epsg()))\n",
    "\n",
    "#                     if item_name == r'B01_x4210162.5_y3367662.5.tif':\n",
    "\n",
    "#                         # convert to dataset\n",
    "#                         dad = da.to_dataset()\n",
    "\n",
    "#                         # add all attributes (again)\n",
    "#                         for attr_name, attr_val in cur_meta.items():\n",
    "#                             if attr_name == 'PROVIDERS':\n",
    "#                                 attr_val = json.dumps(attr_val)\n",
    "#                             if attr_name == \"MEDIA_TYPE\": # change media type to tiff, leave the rest as is\n",
    "#                                 attr_val = \"IMAGE/TIFF\"\n",
    "#                             dad.attrs[attr_name] = attr_val\n",
    "\n",
    "#                         dad.attrs['Conventions'] = \"CF-1.8\"\n",
    "\n",
    "#                         # make parent dir if not exists\n",
    "#                         outpath = cur_dir.joinpath(item_name)\n",
    "#                         outpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#                         # export file\n",
    "#                         # dad.rio.to_raster(outpath, driver=\"COG\")\n",
    "\n",
    "#                         break\n",
    "#                     if item_name == r'B01_x4210162.5_y3367662.5.tif': break\n",
    "\n",
    "#                 if item_name == r'B01_x4210162.5_y3367662.5.tif': break\n",
    "\n",
    "#             if item_name == r'B01_x4210162.5_y3367662.5.tif': break\n",
    "\n",
    "#         if item_name == r'B01_x4210162.5_y3367662.5.tif': break\n",
    "\n",
    "#     if item_name == r'B01_x4210162.5_y3367662.5.tif': break\n",
    "                    # set overwrite is false because tifs should be unique\n",
    "                    # try:\n",
    "                    #     write_cog(da, fname=outpath, overwrite=False).compute()\n",
    "                    # except OSError as e:\n",
    "                    #     continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RasterioIOError",
     "evalue": "p:/11207608-coclico/FULLTRACK_DATA/WP4/cogs/HIGH_DEFENDED_MAPS/RP/1000/B01_x4210162.5_y3367662.5.tif: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\soest\\AppData\\Local\\mambaforge\\envs\\coclico\\Lib\\site-packages\\xarray\\backends\\file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\soest\\AppData\\Local\\mambaforge\\envs\\coclico\\Lib\\site-packages\\xarray\\backends\\lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: [<function open at 0x000001857B074AE0>, ('p:\\\\11207608-coclico\\\\FULLTRACK_DATA\\\\WP4\\\\cogs\\\\HIGH_DEFENDED_MAPS\\\\RP\\\\1000\\\\B01_x4210162.5_y3367662.5.tif',), 'r', (('sharing', False),), 'e32dfef0-88e2-4ca4-ab7f-348d3fa42ebf']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mrasterio\\_base.pyx:310\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mrasterio\\_base.pyx:221\u001b[0m, in \u001b[0;36mrasterio._base.open_dataset\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mrasterio\\_err.pyx:221\u001b[0m, in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: p:/11207608-coclico/FULLTRACK_DATA/WP4/cogs/HIGH_DEFENDED_MAPS/RP/1000/B01_x4210162.5_y3367662.5.tif: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check if resulting tif has same pixel values, for a random pixel with data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mrioxarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_rasterio\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mp:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m11207608-coclico\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mFULLTRACK_DATA\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mWP4\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcogs\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mHIGH_DEFENDED_MAPS\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mRP\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1000\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mB01_x4210162.5_y3367662.5.tif\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(test\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1400\u001b[39m,\u001b[38;5;241m2000\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(da\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m1400\u001b[39m,\u001b[38;5;241m2000\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\soest\\AppData\\Local\\mambaforge\\envs\\coclico\\Lib\\site-packages\\rioxarray\\_io.py:1124\u001b[0m, in \u001b[0;36mopen_rasterio\u001b[1;34m(filename, parse_coordinates, chunks, cache, lock, masked, mask_and_scale, variable, group, default_name, decode_times, decode_timedelta, band_as_variable, **open_kwargs)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1123\u001b[0m         manager \u001b[38;5;241m=\u001b[39m URIManager(file_opener, filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m=\u001b[39mopen_kwargs)\n\u001b[1;32m-> 1124\u001b[0m     riods \u001b[38;5;241m=\u001b[39m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1125\u001b[0m     captured_warnings \u001b[38;5;241m=\u001b[39m rio_warnings\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# raise the NotGeoreferencedWarning if applicable\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\soest\\AppData\\Local\\mambaforge\\envs\\coclico\\Lib\\site-packages\\xarray\\backends\\file_manager.py:193\u001b[0m, in \u001b[0;36mCachingFileManager.acquire\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Acquire a file object from the manager.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    A new file is only opened if it has expired from the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m        An open file object, as returned by ``opener(*args, **kwargs)``.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m     file, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[1;32mc:\\Users\\soest\\AppData\\Local\\mambaforge\\envs\\coclico\\Lib\\site-packages\\xarray\\backends\\file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[1;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\soest\\AppData\\Local\\mambaforge\\envs\\coclico\\Lib\\site-packages\\rasterio\\env.py:451\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    448\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\soest\\AppData\\Local\\mambaforge\\envs\\coclico\\Lib\\site-packages\\rasterio\\__init__.py:304\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m path \u001b[38;5;241m=\u001b[39m _parse_path(raw_dataset_path)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 304\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    306\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m get_writer_for_path(path, driver\u001b[38;5;241m=\u001b[39mdriver)(\n\u001b[0;32m    307\u001b[0m         path, mode, driver\u001b[38;5;241m=\u001b[39mdriver, sharing\u001b[38;5;241m=\u001b[39msharing, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    308\u001b[0m     )\n",
      "File \u001b[1;32mrasterio\\_base.pyx:312\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRasterioIOError\u001b[0m: p:/11207608-coclico/FULLTRACK_DATA/WP4/cogs/HIGH_DEFENDED_MAPS/RP/1000/B01_x4210162.5_y3367662.5.tif: No such file or directory"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 15;\n                var nbb_unformatted_code = \"# Check if resulting tif has same pixel values, for a random pixel with data\\ntest = rioxarray.open_rasterio(r'p:\\\\11207608-coclico\\\\FULLTRACK_DATA\\\\WP4\\\\cogs\\\\HIGH_DEFENDED_MAPS\\\\RP\\\\1000\\\\B01_x4210162.5_y3367662.5.tif')\\nprint(test.values[0][1400,2000])\\nprint(da.values[1400,2000])\";\n                var nbb_formatted_code = \"# Check if resulting tif has same pixel values, for a random pixel with data\\ntest = rioxarray.open_rasterio(\\n    r\\\"p:\\\\11207608-coclico\\\\FULLTRACK_DATA\\\\WP4\\\\cogs\\\\HIGH_DEFENDED_MAPS\\\\RP\\\\1000\\\\B01_x4210162.5_y3367662.5.tif\\\"\\n)\\nprint(test.values[0][1400, 2000])\\nprint(da.values[1400, 2000])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if resulting tif has same pixel values, for a random pixel with data\n",
    "test = rioxarray.open_rasterio(r'p:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs\\HIGH_DEFENDED_MAPS\\RP\\1000\\B01_x4210162.5_y3367662.5.tif')\n",
    "print(test.values[0][1400,2000])\n",
    "print(da.values[1400,2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to open: Mean_spring_tide_HD.json\n",
      "trying to open: RP1000_HD.json\n",
      "trying to open: RP100_HD.json\n",
      "trying to open: RP1_HD.json\n",
      "trying to open: SLR_High-End_2100_subs_2050_HD.json\n",
      "trying to open: SLR_High-End_2150_subs_2050_HD.json\n",
      "trying to open: SLR_SSP126_2100_subs_2050_HD.json\n",
      "trying to open: SLR_SSP245_2050_subs_HD.json\n",
      "trying to open: SLR_SSP245_2100_subs_2050_HD.json\n",
      "trying to open: SLR_SSP585_2030_subs_HD.json\n",
      "trying to open: SLR_SSP585_2050_subs_HD.json\n",
      "trying to open: SLR_SSP585_2100_subs_2050_HD.json\n",
      "trying to open: Mean_spring_tide_LD.json\n",
      "trying to open: RP1000_LD.json\n",
      "trying to open: RP100_LD.json\n",
      "trying to open: RP1_LD.json\n",
      "trying to open: SLR_High-End_2100_subs_2050_LD.json\n",
      "trying to open: SLR_High-End_2150_subs_2050_LD.json\n",
      "trying to open: SLR_SSP126_2100_subs_2050_LD.json\n",
      "trying to open: SLR_SSP245_2050_subs_LD.json\n",
      "trying to open: SLR_SSP245_2100_subs_2050_LD.json\n",
      "trying to open: SLR_SSP585_2030_subs_LD.json\n",
      "trying to open: SLR_SSP585_2050_subs_LD.json\n",
      "trying to open: SLR_SSP585_2100_subs_2050_LD.json\n",
      "trying to open: High_tide.json\n",
      "trying to open: RP1.json\n",
      "trying to open: RP100.json\n",
      "trying to open: RP1000.json\n",
      "trying to open: SLR_High-End_2100_subs_2050.json\n",
      "trying to open: SLR_High-End_2150_subs_2050.json\n",
      "trying to open: SLR_SSP126_2100_subs_2050.json\n",
      "trying to open: SLR_SSP245_2050_subs.json\n",
      "trying to open: SLR_SSP245_2100_subs_2050.json\n",
      "trying to open: SLR_SSP585_2030_subs.json\n",
      "trying to open: SLR_SSP585_2050_subs.json\n",
      "trying to open: SLR_SSP585_2100_subs_2050.json\n",
      "All .json files are working\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 16;\n                var nbb_unformatted_code = \"# NOTE: Not all meta_data.json files were correct and will break the loop. \\n# CHECK if all meta_data \\n\\nfor map_type in map_types:\\n\\n    # Get list of original tif's per map_type\\n    tif_list = glob.glob(str(ds_dir.joinpath(\\\"data\\\", map_type,\\\"*.tif\\\")))\\n    \\n    for cur_path, cur_tif in zip(path_list, tif_list):\\n        \\n        print('trying to open: ' + str(os.path.basename(cur_tif.replace('tif','json'))))\\n        \\n        # Load meta data\\n        cur_meta_data = open(os.path.join(os.path.dirname(cur_tif),'Metadata',os.path.basename(cur_tif.replace('tif','json'))))\\n        cur_meta = json.load(cur_meta_data)\\n\\n        if map_type == map_types[-1] and cur_tif == tif_list[-1]:\\n            print('All .json files are working')\";\n                var nbb_formatted_code = \"# NOTE: Not all meta_data.json files were correct and will break the loop.\\n# CHECK if all meta_data\\n\\nfor map_type in map_types:\\n    # Get list of original tif's per map_type\\n    tif_list = glob.glob(str(ds_dir.joinpath(\\\"data\\\", map_type, \\\"*.tif\\\")))\\n\\n    for cur_path, cur_tif in zip(path_list, tif_list):\\n        print(\\n            \\\"trying to open: \\\" + str(os.path.basename(cur_tif.replace(\\\"tif\\\", \\\"json\\\")))\\n        )\\n\\n        # Load meta data\\n        cur_meta_data = open(\\n            os.path.join(\\n                os.path.dirname(cur_tif),\\n                \\\"Metadata\\\",\\n                os.path.basename(cur_tif.replace(\\\"tif\\\", \\\"json\\\")),\\n            )\\n        )\\n        cur_meta = json.load(cur_meta_data)\\n\\n        if map_type == map_types[-1] and cur_tif == tif_list[-1]:\\n            print(\\\"All .json files are working\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: Not all meta_data.json files were correct and will break the loop. \n",
    "# CHECK if all meta_data \n",
    "\n",
    "for map_type in map_types:\n",
    "\n",
    "    # Get list of original tif's per map_type\n",
    "    tif_list = glob.glob(str(ds_dir.joinpath(\"data\", map_type,\"*.tif\")))\n",
    "    \n",
    "    for cur_path, cur_tif in zip(path_list, tif_list):\n",
    "        \n",
    "        print('trying to open: ' + str(os.path.basename(cur_tif.replace('tif','json'))))\n",
    "        \n",
    "        # Load meta data\n",
    "        cur_meta_data = open(os.path.join(os.path.dirname(cur_tif),'Metadata',os.path.basename(cur_tif.replace('tif','json'))))\n",
    "        cur_meta = json.load(cur_meta_data)\n",
    "\n",
    "        if map_type == map_types[-1] and cur_tif == tif_list[-1]:\n",
    "            print('All .json files are working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOW_DEFENDED_MAPS', 'UNDEFENDED_MAPS']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 17;\n                var nbb_unformatted_code = \"map_types[1:]\";\n                var nbb_formatted_code = \"map_types[1:]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "map_types[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently working on: Mean_spring_tide P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\Mean_spring_tide_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\LOW_DEFENDED_MAPS\\Mean_spring_tide\n",
      "currently working on: RP\\1 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\RP1000_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\LOW_DEFENDED_MAPS\\RP\\1\n",
      "currently working on: RP\\100 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\RP100_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\LOW_DEFENDED_MAPS\\RP\\100\n",
      "currently working on: RP\\1000 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\RP1_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\LOW_DEFENDED_MAPS\\RP\\1000\n",
      "currently working on: SLR\\High_end\\2100 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_High-End_2100_subs_2050_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\LOW_DEFENDED_MAPS\\SLR\\High_end\\2100\n",
      "currently working on: SLR\\High_end\\2150 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_High-End_2150_subs_2050_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\LOW_DEFENDED_MAPS\\SLR\\High_end\\2150\n",
      "currently working on: SLR\\SSP126\\2100 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP126_2100_subs_2050_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\LOW_DEFENDED_MAPS\\SLR\\SSP126\\2100\n",
      "currently working on: SLR\\SSP245\\2050 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP245_2050_subs_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\LOW_DEFENDED_MAPS\\SLR\\SSP245\\2050\n",
      "currently working on: SLR\\SSP245\\2100 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP245_2100_subs_2050_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\LOW_DEFENDED_MAPS\\SLR\\SSP245\\2100\n",
      "currently working on: SLR\\SSP585\\2030 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP585_2030_subs_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\LOW_DEFENDED_MAPS\\SLR\\SSP585\\2030\n",
      "currently working on: SLR\\SSP585\\2050 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP585_2050_subs_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\LOW_DEFENDED_MAPS\\SLR\\SSP585\\2050\n",
      "currently working on: SLR\\SSP585\\2100 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP585_2100_subs_2050_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\LOW_DEFENDED_MAPS\\SLR\\SSP585\\2100\n",
      "currently working on: Mean_spring_tide P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\High_tide.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\UNDEFENDED_MAPS\\Mean_spring_tide\n",
      "currently working on: RP\\1000 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\RP1.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\UNDEFENDED_MAPS\\RP\\1000\n",
      "currently working on: RP\\100 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\RP100.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\UNDEFENDED_MAPS\\RP\\100\n",
      "currently working on: RP\\1 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\RP1000.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\UNDEFENDED_MAPS\\RP\\1\n",
      "currently working on: SLR\\High_end\\2100 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_High-End_2100_subs_2050.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\UNDEFENDED_MAPS\\SLR\\High_end\\2100\n",
      "currently working on: SLR\\High_end\\2150 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_High-End_2150_subs_2050.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\UNDEFENDED_MAPS\\SLR\\High_end\\2150\n",
      "currently working on: SLR\\SSP126\\2100 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP126_2100_subs_2050.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\UNDEFENDED_MAPS\\SLR\\SSP126\\2100\n",
      "currently working on: SLR\\SSP245\\2050 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP245_2050_subs.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\UNDEFENDED_MAPS\\SLR\\SSP245\\2050\n",
      "currently working on: SLR\\SSP245\\2100 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP245_2100_subs_2050.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\UNDEFENDED_MAPS\\SLR\\SSP245\\2100\n",
      "currently working on: SLR\\SSP585\\2030 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP585_2030_subs.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\UNDEFENDED_MAPS\\SLR\\SSP585\\2030\n",
      "currently working on: SLR\\SSP585\\2050 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP585_2050_subs.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\UNDEFENDED_MAPS\\SLR\\SSP585\\2050\n",
      "currently working on: SLR\\SSP585\\2100 P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP585_2100_subs_2050.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_final\\UNDEFENDED_MAPS\\SLR\\SSP585\\2100\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 82;\n                var nbb_unformatted_code = \"# DO THE WORK\\n\\n# Iterate over the original tif files\\nfor map_type in map_types[1:]:\\n    \\n    # Get list of original tif's per map_type\\n    tif_list = glob.glob(str(ds_dir.joinpath(\\\"data\\\", map_type,\\\"*.tif\\\")))\\n\\n    if map_type == 'UNDEFENDED_MAPS':\\n        # Reverse folder structure in case of undefended maps\\n        folder_structure['RP'].reverse()\\n        path_list_rev = get_paths(folder_structure) \\n        path_list = path_list_rev\\n\\n    for cur_path, cur_tif in zip(path_list, tif_list):\\n\\n        print('currently working on: '+str(cur_path)+' '+str(cur_tif))\\n        \\n        cur_dir = pathlib.Path(os.path.join(cog_dirs,map_type,cur_path))\\n        cur_dir.mkdir(parents=True,exist_ok=True)\\n    \\n\\n        print(cur_dir)\\n\\n        # fm = rioxarray.open_rasterio(\\n        #     cur_tif, mask_and_scale=False\\n        # )  # .isel({\\\"x\\\":slice(0, 40000), \\\"y\\\":slice(0, 40000)})\\n        # fm = fm.assign_coords(band=(\\\"band\\\", [f\\\"B{k+1:02}\\\" for k in range(1)])) # NOTE: hard coded to 1, because one band\\n        # fm = fm.to_dataset(\\\"band\\\")\\n\\n        # # chunk size \\n        # chunk_size = 2**15 # 16384, which is large, but OK for int8 datatype.\\n\\n        # fm_chunked = fm.chunk({\\\"x\\\": chunk_size, \\\"y\\\": chunk_size})\\n\\n        # num_x_chunks = math.ceil(fm_chunked.dims[\\\"x\\\"] / chunk_size)\\n        # num_y_chunks = math.ceil(fm_chunked.dims[\\\"y\\\"] / chunk_size)\\n\\n        # # Load meta data\\n        # cur_meta_data = open(os.path.join(os.path.dirname(cur_tif),'Metadata',os.path.basename(cur_tif.replace('tif','json'))))\\n        # cur_meta = json.load(cur_meta_data)\\n\\n        # for x_slice in generate_slices(num_x_chunks, chunk_size):\\n        #     for y_slice in generate_slices(num_y_chunks, chunk_size):\\n        #         chunk = fm_chunked.isel(x=x_slice, y=y_slice)\\n\\n        #         chunk = chunk.assign_coords(time=pd.Timestamp(2024, 3, 18).isoformat())\\n\\n        #         for var in chunk:\\n\\n        #             da = chunk[var]\\n\\n        #             da = (\\n        #                 da.where(da != 3.3999999521443642e+38,-9999)\\n        #                 .astype(\\\"float32\\\")\\n        #                 .rio.write_nodata(-9999)\\n        #                 .rio.set_spatial_dims(x_dim=\\\"x\\\", y_dim=\\\"y\\\")\\n        #             )\\n\\n        #             item_name = name_block(\\n        #                 da,\\n        #                 storage_prefix=\\\"\\\",\\n        #                 name_prefix=da.name,\\n        #                 include_band=None, \\n        #                 time_dim=False,\\n        #                 x_dim=\\\"x\\\",\\n        #                 y_dim=\\\"y\\\",\\n        #             )\\n\\n        #             # hacky fix to get rid of the espg=None string\\n        #             if \\\"None\\\" in item_name:\\n        #                 item_name = item_name.replace(\\\"None\\\", str(rasterio.crs.CRS(da.rio.crs).to_epsg()))\\n\\n        #             print(item_name)\\n\\n        #             # convert to dataset\\n        #             dad = da.to_dataset()\\n\\n        #             # add all attributes (again)\\n        #             for attr_name, attr_val in cur_meta.items():\\n        #                 if attr_name == 'PROVIDERS':\\n        #                     attr_val = json.dumps(attr_val)\\n        #                 if attr_name == \\\"MEDIA_TYPE\\\": # change media type to tiff, leave the rest as is\\n        #                     attr_val = \\\"IMAGE/TIFF\\\"\\n        #                 dad.attrs[attr_name] = attr_val\\n\\n        #             dad.attrs['Conventions'] = \\\"CF-1.8\\\"\\n\\n        #             # make parent dir if not exists\\n\\n        #             outpath = cur_dir.joinpath(item_name)\\n        #             outpath.parent.mkdir(parents=True, exist_ok=True)\\n\\n        #             # export file\\n        #             dad.rio.to_raster(outpath, compress=\\\"DEFLATE\\\", driver=\\\"COG\\\")\\n\\n                    # set overwrite is false because tifs should be unique\\n                    # try:\\n                    #     write_cog(da, fname=outpath, overwrite=False).compute()\\n                    # except OSError as e:\\n                    #     continue\";\n                var nbb_formatted_code = \"# DO THE WORK\\n\\n# Iterate over the original tif files\\nfor map_type in map_types[1:]:\\n    # Get list of original tif's per map_type\\n    tif_list = glob.glob(str(ds_dir.joinpath(\\\"data\\\", map_type, \\\"*.tif\\\")))\\n\\n    if map_type == \\\"UNDEFENDED_MAPS\\\":\\n        # Reverse folder structure in case of undefended maps\\n        folder_structure[\\\"RP\\\"].reverse()\\n        path_list_rev = get_paths(folder_structure)\\n        path_list = path_list_rev\\n\\n    for cur_path, cur_tif in zip(path_list, tif_list):\\n        print(\\\"currently working on: \\\" + str(cur_path) + \\\" \\\" + str(cur_tif))\\n\\n        cur_dir = pathlib.Path(os.path.join(cog_dirs, map_type, cur_path))\\n        cur_dir.mkdir(parents=True, exist_ok=True)\\n\\n        print(cur_dir)\\n\\n        # fm = rioxarray.open_rasterio(\\n        #     cur_tif, mask_and_scale=False\\n        # )  # .isel({\\\"x\\\":slice(0, 40000), \\\"y\\\":slice(0, 40000)})\\n        # fm = fm.assign_coords(band=(\\\"band\\\", [f\\\"B{k+1:02}\\\" for k in range(1)])) # NOTE: hard coded to 1, because one band\\n        # fm = fm.to_dataset(\\\"band\\\")\\n\\n        # # chunk size\\n        # chunk_size = 2**15 # 16384, which is large, but OK for int8 datatype.\\n\\n        # fm_chunked = fm.chunk({\\\"x\\\": chunk_size, \\\"y\\\": chunk_size})\\n\\n        # num_x_chunks = math.ceil(fm_chunked.dims[\\\"x\\\"] / chunk_size)\\n        # num_y_chunks = math.ceil(fm_chunked.dims[\\\"y\\\"] / chunk_size)\\n\\n        # # Load meta data\\n        # cur_meta_data = open(os.path.join(os.path.dirname(cur_tif),'Metadata',os.path.basename(cur_tif.replace('tif','json'))))\\n        # cur_meta = json.load(cur_meta_data)\\n\\n        # for x_slice in generate_slices(num_x_chunks, chunk_size):\\n        #     for y_slice in generate_slices(num_y_chunks, chunk_size):\\n        #         chunk = fm_chunked.isel(x=x_slice, y=y_slice)\\n\\n        #         chunk = chunk.assign_coords(time=pd.Timestamp(2024, 3, 18).isoformat())\\n\\n        #         for var in chunk:\\n\\n        #             da = chunk[var]\\n\\n        #             da = (\\n        #                 da.where(da != 3.3999999521443642e+38,-9999)\\n        #                 .astype(\\\"float32\\\")\\n        #                 .rio.write_nodata(-9999)\\n        #                 .rio.set_spatial_dims(x_dim=\\\"x\\\", y_dim=\\\"y\\\")\\n        #             )\\n\\n        #             item_name = name_block(\\n        #                 da,\\n        #                 storage_prefix=\\\"\\\",\\n        #                 name_prefix=da.name,\\n        #                 include_band=None,\\n        #                 time_dim=False,\\n        #                 x_dim=\\\"x\\\",\\n        #                 y_dim=\\\"y\\\",\\n        #             )\\n\\n        #             # hacky fix to get rid of the espg=None string\\n        #             if \\\"None\\\" in item_name:\\n        #                 item_name = item_name.replace(\\\"None\\\", str(rasterio.crs.CRS(da.rio.crs).to_epsg()))\\n\\n        #             print(item_name)\\n\\n        #             # convert to dataset\\n        #             dad = da.to_dataset()\\n\\n        #             # add all attributes (again)\\n        #             for attr_name, attr_val in cur_meta.items():\\n        #                 if attr_name == 'PROVIDERS':\\n        #                     attr_val = json.dumps(attr_val)\\n        #                 if attr_name == \\\"MEDIA_TYPE\\\": # change media type to tiff, leave the rest as is\\n        #                     attr_val = \\\"IMAGE/TIFF\\\"\\n        #                 dad.attrs[attr_name] = attr_val\\n\\n        #             dad.attrs['Conventions'] = \\\"CF-1.8\\\"\\n\\n        #             # make parent dir if not exists\\n\\n        #             outpath = cur_dir.joinpath(item_name)\\n        #             outpath.parent.mkdir(parents=True, exist_ok=True)\\n\\n        #             # export file\\n        #             dad.rio.to_raster(outpath, compress=\\\"DEFLATE\\\", driver=\\\"COG\\\")\\n\\n        # set overwrite is false because tifs should be unique\\n        # try:\\n        #     write_cog(da, fname=outpath, overwrite=False).compute()\\n        # except OSError as e:\\n        #     continue\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO THE WORK\n",
    "\n",
    "# Iterate over the original tif files\n",
    "for map_type in map_types[1:]:\n",
    "    \n",
    "    # Get list of original tif's per map_type\n",
    "    tif_list = glob.glob(str(ds_dir.joinpath(\"data\", map_type,\"*.tif\")))\n",
    "\n",
    "    for cur_path, cur_tif in zip(path_list, tif_list):\n",
    "\n",
    "        print('currently working on: '+str(cur_path)+' '+str(cur_tif))\n",
    "        \n",
    "        cur_dir = pathlib.Path(os.path.join(cog_dirs,map_type,cur_path))\n",
    "        cur_dir.mkdir(parents=True,exist_ok=True)\n",
    "    \n",
    "\n",
    "        print(cur_dir)\n",
    "\n",
    "        # fm = rioxarray.open_rasterio(\n",
    "        #     cur_tif, mask_and_scale=False\n",
    "        # )  # .isel({\"x\":slice(0, 40000), \"y\":slice(0, 40000)})\n",
    "        # fm = fm.assign_coords(band=(\"band\", [f\"B{k+1:02}\" for k in range(1)])) # NOTE: hard coded to 1, because one band\n",
    "        # fm = fm.to_dataset(\"band\")\n",
    "\n",
    "        # # chunk size \n",
    "        # chunk_size = 2**15 # 16384, which is large, but OK for int8 datatype.\n",
    "\n",
    "        # fm_chunked = fm.chunk({\"x\": chunk_size, \"y\": chunk_size})\n",
    "\n",
    "        # num_x_chunks = math.ceil(fm_chunked.dims[\"x\"] / chunk_size)\n",
    "        # num_y_chunks = math.ceil(fm_chunked.dims[\"y\"] / chunk_size)\n",
    "\n",
    "        # # Load meta data\n",
    "        # cur_meta_data = open(os.path.join(os.path.dirname(cur_tif),'Metadata',os.path.basename(cur_tif.replace('tif','json'))))\n",
    "        # cur_meta = json.load(cur_meta_data)\n",
    "\n",
    "        # for x_slice in generate_slices(num_x_chunks, chunk_size):\n",
    "        #     for y_slice in generate_slices(num_y_chunks, chunk_size):\n",
    "        #         chunk = fm_chunked.isel(x=x_slice, y=y_slice)\n",
    "\n",
    "        #         chunk = chunk.assign_coords(time=pd.Timestamp(2024, 3, 18).isoformat())\n",
    "\n",
    "        #         for var in chunk:\n",
    "\n",
    "        #             da = chunk[var]\n",
    "\n",
    "        #             da = (\n",
    "        #                 da.where(da != 3.3999999521443642e+38,-9999)\n",
    "        #                 .astype(\"float32\")\n",
    "        #                 .rio.write_nodata(-9999)\n",
    "        #                 .rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\")\n",
    "        #             )\n",
    "\n",
    "        #             item_name = name_block(\n",
    "        #                 da,\n",
    "        #                 storage_prefix=\"\",\n",
    "        #                 name_prefix=da.name,\n",
    "        #                 include_band=None, \n",
    "        #                 time_dim=False,\n",
    "        #                 x_dim=\"x\",\n",
    "        #                 y_dim=\"y\",\n",
    "        #             )\n",
    "\n",
    "        #             # hacky fix to get rid of the espg=None string\n",
    "        #             if \"None\" in item_name:\n",
    "        #                 item_name = item_name.replace(\"None\", str(rasterio.crs.CRS(da.rio.crs).to_epsg()))\n",
    "\n",
    "        #             print(item_name)\n",
    "\n",
    "        #             # convert to dataset\n",
    "        #             dad = da.to_dataset()\n",
    "\n",
    "        #             # add all attributes (again)\n",
    "        #             for attr_name, attr_val in cur_meta.items():\n",
    "        #                 if attr_name == 'PROVIDERS':\n",
    "        #                     attr_val = json.dumps(attr_val)\n",
    "        #                 if attr_name == \"MEDIA_TYPE\": # change media type to tiff, leave the rest as is\n",
    "        #                     attr_val = \"IMAGE/TIFF\"\n",
    "        #                 dad.attrs[attr_name] = attr_val\n",
    "\n",
    "        #             dad.attrs['Conventions'] = \"CF-1.8\"\n",
    "\n",
    "        #             # make parent dir if not exists\n",
    "\n",
    "        #             outpath = cur_dir.joinpath(item_name)\n",
    "        #             outpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        #             # export file\n",
    "        #             dad.rio.to_raster(outpath, compress=\"DEFLATE\", driver=\"COG\")\n",
    "\n",
    "                    # set overwrite is false because tifs should be unique\n",
    "                    # try:\n",
    "                    #     write_cog(da, fname=outpath, overwrite=False).compute()\n",
    "                    # except OSError as e:\n",
    "                    #     continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\Mean_spring_tide_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\RP_1000_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\RP_100_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\RP_1_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_High_end_2100_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_High_end_2150_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP126_2100_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP245_2050_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP245_2100_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP585_2030_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP585_2050_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP585_2100_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\Mean_spring_tide_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\RP_1000_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\RP_100_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\RP_1_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_High_end_2100_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_High_end_2150_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP126_2100_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP245_2050_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP245_2100_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP585_2030_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP585_2050_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP585_2100_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\Mean_spring_tide.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\RP_1000.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\RP_100.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\RP_1.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_High_end_2100.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_High_end_2150.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP126_2100.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP245_2050.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP245_2100.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP585_2030.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP585_2050.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP585_2100.tif\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 166;\n                var nbb_unformatted_code = \"path_list\\n\\nmap_suffixes = {'HIGH_DEFENDED_MAPS': '_HD',\\n                'LOW_DEFENDED_MAPS':  '_LD',\\n                'UNDEFENDED_MAPS':    ''} \\n\\nfor map_type in map_types:\\n    for cur_path in path_list:\\n\\n        map_suffix = map_suffixes[map_type]\\n\\n        cur_tif_fn = cur_path + map_suffix + '.tif'\\n        \\n        cur_tif_fn = cur_tif_fn.replace('\\\\\\\\','_')\\n        cur_tif = ds_dir.joinpath('data',map_type,cur_tif_fn)\\n        print(cur_tif)\";\n                var nbb_formatted_code = \"path_list\\n\\nmap_suffixes = {\\n    \\\"HIGH_DEFENDED_MAPS\\\": \\\"_HD\\\",\\n    \\\"LOW_DEFENDED_MAPS\\\": \\\"_LD\\\",\\n    \\\"UNDEFENDED_MAPS\\\": \\\"\\\",\\n}\\n\\nfor map_type in map_types:\\n    for cur_path in path_list:\\n        map_suffix = map_suffixes[map_type]\\n\\n        cur_tif_fn = cur_path + map_suffix + \\\".tif\\\"\\n\\n        cur_tif_fn = cur_tif_fn.replace(\\\"\\\\\\\\\\\", \\\"_\\\")\\n        cur_tif = ds_dir.joinpath(\\\"data\\\", map_type, cur_tif_fn)\\n        print(cur_tif)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_list\n",
    "\n",
    "map_suffixes = {'HIGH_DEFENDED_MAPS': '_HD',\n",
    "                'LOW_DEFENDED_MAPS':  '_LD',\n",
    "                'UNDEFENDED_MAPS':    ''} \n",
    "\n",
    "for map_type in map_types:\n",
    "    for cur_path in path_list:\n",
    "\n",
    "        map_suffix = map_suffixes[map_type]\n",
    "\n",
    "        cur_tif_fn = cur_path + map_suffix + '.tif'\n",
    "        \n",
    "        cur_tif_fn = cur_tif_fn.replace('\\\\','_')\n",
    "        cur_tif = ds_dir.joinpath('data',map_type,cur_tif_fn)\n",
    "        print(cur_tif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\Mean_spring_tide_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\RP1000_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\RP100_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\RP1_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP126_2100_subs_2050_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP245_2050_subs_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP245_2100_subs_2050_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP245_2100_subs_2050_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP585_2030_subs_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP585_2050_subs_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP585_2100_subs_2050_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\HIGH_DEFENDED_MAPS\\SLR_SSP585_2100_subs_2050_HD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\Mean_spring_tide_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\RP1000_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\RP100_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\RP1_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP126_2100_subs_2050_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP245_2050_subs_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP245_2100_subs_2050_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP245_2100_subs_2050_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP585_2030_subs_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP585_2050_subs_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP585_2100_subs_2050_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\LOW_DEFENDED_MAPS\\SLR_SSP585_2100_subs_2050_LD.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP126_2100_subs_2050.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP245_2050_subs.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP245_2100_subs_2050.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP245_2100_subs_2050.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP585_2030_subs.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP585_2050_subs.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP585_2100_subs_2050.tif\n",
      "P:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\data\\UNDEFENDED_MAPS\\SLR_SSP585_2100_subs_2050.tif\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 169;\n                var nbb_unformatted_code = \"for map_type in map_types:\\n    # Get list of original tif's per map_type\\n    tif_list = glob.glob(str(ds_dir.joinpath(\\\"data\\\", map_type,\\\"*.tif\\\")))\\n    for cur_path in path_list:\\n\\n        cur_tif_parts = list(pathlib.Path(cur_path).parts)\\n        \\n        if cur_tif_parts[0] == 'RP':\\n            cur_tif_parts[-1] = cur_tif_parts[-1] + '_'\\n\\n        for cur_tif in tif_list:\\n            if all([part in cur_tif for part in cur_tif_parts]):\\n                print(cur_tif)\";\n                var nbb_formatted_code = \"for map_type in map_types:\\n    # Get list of original tif's per map_type\\n    tif_list = glob.glob(str(ds_dir.joinpath(\\\"data\\\", map_type, \\\"*.tif\\\")))\\n    for cur_path in path_list:\\n        cur_tif_parts = list(pathlib.Path(cur_path).parts)\\n\\n        if cur_tif_parts[0] == \\\"RP\\\":\\n            cur_tif_parts[-1] = cur_tif_parts[-1] + \\\"_\\\"\\n\\n        for cur_tif in tif_list:\\n            if all([part in cur_tif for part in cur_tif_parts]):\\n                print(cur_tif)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for map_type in map_types:\n",
    "    # Get list of original tif's per map_type\n",
    "    tif_list = glob.glob(str(ds_dir.joinpath(\"data\", map_type,\"*.tif\")))\n",
    "    for cur_path in path_list:\n",
    "\n",
    "        cur_tif_parts = list(pathlib.Path(cur_path).parts)\n",
    "        \n",
    "        if cur_tif_parts[0] == 'RP':\n",
    "            cur_tif_parts[-1] = cur_tif_parts[-1] + '_'\n",
    "\n",
    "        for cur_tif in tif_list:\n",
    "            if all([part in cur_tif for part in cur_tif_parts]):\n",
    "                print(cur_tif)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check if min value corresponds to expected min value\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m np\u001b[38;5;241m.\u001b[39mmin(\u001b[43mdad\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB01\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dad' is not defined"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 56;\n                var nbb_unformatted_code = \"# Check if min value corresponds to expected min value\\nnp.min(dad['B01'].values)\";\n                var nbb_formatted_code = \"# Check if min value corresponds to expected min value\\nnp.min(dad[\\\"B01\\\"].values)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if min value corresponds to expected min value\n",
    "np.min(dad['B01'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 14;\n                var nbb_unformatted_code = \"## Check data\\n\\ntest_cog_f = r'p:\\\\11207608-coclico\\\\FULLTRACK_DATA\\\\WP4\\\\cogs_test\\\\HIGH_DEFENDED_MAPS\\\\RP\\\\1000\\\\B01_x4005362.5_y2958062.5.tif'\\n\\nimport rioxarray as rxr\\nds = rxr.open_rasterio(test_cog_f)\";\n                var nbb_formatted_code = \"## Check data\\n\\ntest_cog_f = r\\\"p:\\\\11207608-coclico\\\\FULLTRACK_DATA\\\\WP4\\\\cogs_test\\\\HIGH_DEFENDED_MAPS\\\\RP\\\\1000\\\\B01_x4005362.5_y2958062.5.tif\\\"\\n\\nimport rioxarray as rxr\\n\\nds = rxr.open_rasterio(test_cog_f)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Check data\n",
    "\n",
    "test_cog_f = r'p:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\cogs_test\\HIGH_DEFENDED_MAPS\\RP\\1000\\B01_x4005362.5_y2958062.5.tif'\n",
    "\n",
    "import rioxarray as rxr\n",
    "ds = rxr.open_rasterio(test_cog_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7783523"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 22;\n                var nbb_unformatted_code = \"np.nanmax(ds.values)\";\n                var nbb_formatted_code = \"np.nanmax(ds.values)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.nanmax(ds.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
