{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of Concept for flood maps\n",
    "\n",
    "This notebook is an attempt at translating the CoCliCo User Story into code. \n",
    "\n",
    "The data used is Coastal Flood Hazard Projections and can be found here: p:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import warnings\n",
    "\n",
    "# import holoviews as hv\n",
    "import cartopy.crs as crs\n",
    "import cartopy.feature as cf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tck\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import pystac_client\n",
    "import xarray as xr\n",
    "import rioxarray as rio\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pystac\n",
    "import pystac_client\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "from pystac.extensions.projection import ProjectionExtension\n",
    "\n",
    "\n",
    "#import colormaps as cmaps\n",
    "import pyam # https://pyam-iamc.readthedocs.io/en/latest/index.html\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict\n",
    "\n",
    "# Import custom functionality\n",
    "from coclicodata.drive_config import p_drive\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open STAC data\n",
    "\n",
    "1st connect to the catalog, \n",
    "2nd retrieve the collection of interest, \n",
    "3rd retrieve one item to see it's contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the URL to STAC catalog in Google Cloud\n",
    "catalog = pystac_client.Client.open(\n",
    "    P\n",
    ")\n",
    "\n",
    "# Retrieve collection from catalog, in this case Coastal Flood Hazard Projections (cfhp)\n",
    "collection = catalog.get_child(id = 'cfhp_all')\n",
    "\n",
    "# Show collection\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a single item for testing\n",
    "test_item = collection.get_item(r'UNDEFENDED_MAPS\\1000\\None\\2010.tif')\n",
    "\n",
    "# Show test item\n",
    "test_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open LAU's from STAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve collection from catalog, in this case Coastal Flood Hazard Projections (cfhp)\n",
    "LAU = catalog.get_child('LAU_CM')\n",
    "\n",
    "# Get the href to the lau data\n",
    "cloud_lau_path = LAU.assets['geoparquet-stac-items'].href\n",
    "\n",
    "# Retrieve actual data using regular pandas, loading with geopandas is very slow\n",
    "lau_data = pd.read_parquet(cloud_lau_path)\n",
    "\n",
    "# Because we load with regular pandas the polygon data needs to be converted from WKB - Well-Known Binary to shapely.Polygon\n",
    "lau_data['geometry'] = lau_data['geometry'].apply(shapely.wkb.loads)\n",
    "\n",
    "# Now convert to geopandas\n",
    "lau_data = gpd.GeoDataFrame(lau_data, geometry='geometry', crs='EPSG:3035')\n",
    "\n",
    "lau_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on the Netherlands as an example\n",
    "lau_NL = lau_data.loc[lau_data[\"CNTR_CODE\"] == \"NL\"] \n",
    "\n",
    "# Select one for testing\n",
    "lau = lau_data.loc[lau_data[\"LAU_NAME\"] == \"Varel, Stadt\"] \n",
    "\n",
    "lau.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve from a single chunk geometry \n",
    "test_chunk = test_item.assets.get(r'UNDEFENDED_MAPS\\1000\\None\\2010\\B01_epsg=3035_x=4005362_y=2958337.tif')\n",
    "\n",
    "# Access the Projection extension on the asset\n",
    "projection = ProjectionExtension.ext(test_chunk)\n",
    "\n",
    "[chunk_bbox] = projection.geometry['coordinates']\n",
    "\n",
    "chunk_geom = shapely.Polygon(chunk_bbox)\n",
    "\n",
    "chunk_geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the chunk geometry to a geodataframe\n",
    "chunk_geom_df = gpd.GeoDataFrame(geometry = [chunk_geom])\n",
    "\n",
    "# Generate figure\n",
    "f, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.axis('equal')\n",
    "\n",
    "# Plot extent of the chunk\n",
    "chunk_geom_df.plot(ax=ax, color='lightgray', edgecolor='black')\n",
    "# Plot the LAU\n",
    "lau.plot(ax=ax, color='gray')\n",
    "\n",
    "# Iterate over LAU's, for testing this is just one LAU\n",
    "for i, cur_lau in lau.iterrows():\n",
    "    \n",
    "    # Check if the LAU intersects with the chunk geometry\n",
    "    if cur_lau['geometry'].intersects(chunk_geom_df['geometry'])[0]:\n",
    "        \n",
    "        print('Working on: ' + str(cur_lau['LAU_NAME']))\n",
    "        \n",
    "        # Load raw band_data dataset \n",
    "        ds = rio.open_rasterio(test_chunk.href, masked = True)\n",
    "\n",
    "        # First clip to bounding box\n",
    "        ds_clip = ds.rio.clip_box(*cur_lau.geometry.bounds)\n",
    "\n",
    "        # Then, clip dataset to match AOI polygon\n",
    "        ds_clip = ds_clip.rio.clip(cur_lau)\n",
    "\n",
    "# Plot data\n",
    "ds_clip.plot(ax=ax, cbar_kwargs={'label': 'Flood depth [m]'})\n",
    "ax.set_title('Clipped LAU with flood depth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "\n",
    "# Amount of noData's within dataset clipped to polygon\n",
    "class flood_stats:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Intialize empty lists\n",
    "        self.all_nans = []\n",
    "        self.all_less05 = []\n",
    "        self.all_more05 = []\n",
    "        self.all_total = []\n",
    "        self.all_flooded = []\n",
    "\n",
    "    def compute(self,ds):\n",
    "\n",
    "        self.ds = ds\n",
    "\n",
    "        # Compute number of pixels with Nan, <0.5m flood depth, and >0.5m flood depth\n",
    "        self.n_nans = np.isnan(self.ds.values).sum()\n",
    "        self.n_less05 = np.array([self.ds.values<0.5]).sum()\n",
    "        self.n_more05 = np.array([self.ds.values>0.5]).sum()\n",
    "\n",
    "        # Sum all to find total number of pixels\n",
    "        self.total = self.n_nans + self.n_less05 + self.n_more05\n",
    "\n",
    "        # Determine fraction of pixels that are flooded\n",
    "        self.flooded = np.divide(np.add(self.n_less05, self.n_more05),self.total)\n",
    "\n",
    "    def add_new_values(self):\n",
    "\n",
    "        # Append values to lists\n",
    "        self.all_nans.append(self.n_nans)\n",
    "        self.all_less05.append(self.n_less05)\n",
    "        self.all_more05.append(self.n_more05)\n",
    "        self.all_total.append(self.total)\n",
    "        self.all_flooded.append(self.flooded)\n",
    "        \n",
    "    def convert2arrays(self):\n",
    "\n",
    "        # Convert all lists to numpy arrays\n",
    "        self.all_nans = np.array(self.all_nans)\n",
    "        self.all_less05 = np.array(self.all_less05)\n",
    "        self.all_more05 = np.array(self.all_more05)\n",
    "        self.all_total = np.array(self.all_total)\n",
    "        self.all_flooded = np.array(self.all_flooded)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    clip_stats = flood_stats()\n",
    "    clip_stats.compute(ds_clip)\n",
    "\n",
    "    print('summed pixels = ' + str(clip_stats.total))\n",
    "    print('original pixels = ' + str(np.size(ds_clip.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set plot style \n",
    "plt.style.use('_mpl-gallery')\n",
    "\n",
    "# Set labels\n",
    "labels = 'no flooding','flooding < 0.5m', 'flooding > 0.5m'\n",
    "# Initiate figure\n",
    "fig, ax = plt.subplots()\n",
    "# Do plotting\n",
    "ax.pie(np.array([clip_stats.n_nans,clip_stats.n_less05,clip_stats.n_more05]), labels = labels)\n",
    "ax.set_title('Percentage flooded = ' + str(round(clip_stats.flooded*100)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set up folder structure\n",
    "folder_structure = {\n",
    "    \"Mean_spring_tide\": [],\n",
    "    \"RP\": [\"1000\", \"100\", \"1\"],\n",
    "    \"SLR\": {\n",
    "        \"High_end\": [\"2100\", \"2150\"],\n",
    "        \"SSP126\": [\"2100\"],\n",
    "        \"SSP245\": [\"2050\", \"2100\"],\n",
    "        \"SSP585\": [\"2030\", \"2050\", \"2100\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_paths(folder_structure, base_dir=''):\n",
    "    \"\"\"Generate paths for a folder structure defined by a dict\"\"\"\n",
    "    paths = []\n",
    "    for key, value in folder_structure.items():\n",
    "        if isinstance(value, dict):\n",
    "            paths.extend(get_paths(value, os.path.join(base_dir, key)))\n",
    "        elif isinstance(value, list):\n",
    "            if value:\n",
    "                for item in value:\n",
    "                    if item != \"\":\n",
    "                        paths.append(os.path.join(base_dir, key, item))\n",
    "            else:\n",
    "                paths.append(os.path.join(base_dir, key))\n",
    "        else:\n",
    "            continue\n",
    "    return paths\n",
    "\n",
    "map_types = [\"HIGH_DEFENDED_MAPS\", \"LOW_DEFENDED_MAPS\", \"UNDEFENDED_MAPS\"]\n",
    "\n",
    "path_list = get_paths(folder_structure)\n",
    "path_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all scenarios\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize flood stats class\n",
    "clip_stats = flood_stats()\n",
    "# Initialize empty list for collecting file paths\n",
    "all_fps = []\n",
    "\n",
    "for item in collection.get_all_items():\n",
    "    # NOTE: for now only focus on low_defended map_type\n",
    "    if map_types[1] in item.id and 'SLR' in str(item.id):\n",
    "\n",
    "        for chunk_title, chunk in item.get_assets().items():\n",
    "\n",
    "            if chunk_title != 'visual':\n",
    "                \n",
    "                # Get item bounding box geometry\n",
    "                [chunk_boundbox] = chunk.extra_fields.get('proj:geometry')['coordinates']\n",
    "                # Transform bounding box coordinates to GeoDataFrame Polygon\n",
    "                chunk_boundbox = gpd.GeoDataFrame(geometry = [shapely.Polygon(chunk_boundbox)])\n",
    "                \n",
    "                # Check if lau intersects with item bounding box\n",
    "                if cur_lau.geometry.intersects(chunk_boundbox.geometry)[0]:\n",
    "            \n",
    "                    print('Working on: ' + str(item.id))\n",
    "\n",
    "                    # Load raw band_data dataset \n",
    "                    ds = rio.open_rasterio(chunk.href, masked = True)\n",
    "\n",
    "                    # Clip dataset to match AOI\n",
    "                    ds_clip = ds.rio.clip(cur_lau)\n",
    "\n",
    "                    # Close dataset to save memory\n",
    "                    del ds\n",
    "\n",
    "                    # Compute flood statistics for each floop_maps\n",
    "                    clip_stats.compute(ds_clip)\n",
    "                    clip_stats.add_new_values()\n",
    "\n",
    "                    # Collect all filepaths\n",
    "                    all_fps.append(item.id)\n",
    "\n",
    "clip_stats.convert2arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of paths to pathlib\n",
    "fps = [Path(fp) for fp in all_fps]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "\n",
    "col_alpha = [0.1,0.35,0.65,0.9]\n",
    "col_scen = 'SSP126', 'SSP245', 'SSP585', 'High_end'\n",
    "scen = []\n",
    "year = np.array([])\n",
    "\n",
    "for i, fp in enumerate(fps):\n",
    "    \n",
    "    cur_scen = fp.parts[2]\n",
    "    scen.append(cur_scen)\n",
    "    \n",
    "    year = np.append(year, int(fp.stem))\n",
    "    \n",
    "    ax.plot(year[i],clip_stats.all_flooded[i],'o',color = 'C1', alpha = col_alpha[col_scen.index(cur_scen)], markeredgecolor = 'black')\n",
    "    ax.set_xlabel('Time [years]')\n",
    "    ax.set_ylabel('percentage flooded [%]')\n",
    "ax.legend(scen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this for all polygons within the current tif file\n",
    "\n",
    "    \n",
    "# Add new columns to LAU dataframe\n",
    "lau_data['n_nans'] = None\n",
    "lau_data['n_less05'] = None\n",
    "lau_data['n_more05'] = None\n",
    "lau_data['total'] = None\n",
    "lau_data['flooded'] = None\n",
    "\n",
    "track = np.empty([])\n",
    "\n",
    "# Iterate over all items\n",
    "for i, item in enumerate(collection.get_all_items()):\n",
    "    \n",
    "    # Match with map type we're working on\n",
    "    if not r\"UNDEFENDED_MAPS\\RP\\1000\" in item.id:\n",
    "        continue\n",
    "\n",
    "    for chunk_title, chunk in item.get_assets().items():\n",
    "\n",
    "        if 'visual' in chunk_title:\n",
    "            continue\n",
    "\n",
    "        if not \"B01_epsg=3035_x=4005362_y=2958337.tif\" in chunk_title:\n",
    "            continue\n",
    "\n",
    "        print('Working on ' + str(item.id) + ' | chunk: ' + pathlib.Path(chunk_title).stem)\n",
    "        # Retreive item geometry\n",
    "        [chunk_geom] = chunk.extra_fields.get('proj:geometry')['coordinates']\n",
    "        # Convert to polygon\n",
    "        chunk_geom_df = gpd.GeoDataFrame(geometry = [shapely.Polygon(chunk_geom)])\n",
    "\n",
    "        # Load raw band_data dataset \n",
    "        ds = rio.open_rasterio(chunk.href, masked = True)\n",
    "\n",
    "        # Iterate over LAU's \n",
    "        for i, cur_lau in lau_data.iterrows():\n",
    "\n",
    "            # Check if LAU intersects with flood map extent\n",
    "            # TODO: change from within to intersects and find a way to deal with bordering tif's \n",
    "            if not cur_lau['geometry'].within(chunk_geom_df['geometry'])[0]:\n",
    "                continue\n",
    "\n",
    "            # First clip to bounding box\n",
    "            ds_clip = ds.rio.clip_box(*cur_lau.geometry.bounds)\n",
    "\n",
    "            # Check if flooded pixels exist within the bounding box\n",
    "            if ds_clip.isnull().all():\n",
    "                continue\n",
    "\n",
    "            track = np.append(track, i)\n",
    "            \n",
    "            # Update user\n",
    "            print('Flooded pixels detected in: ' + str(cur_lau['LAU_NAME']))\n",
    "            # Then, clip dataset to match AOI polygon\n",
    "            ds_clip = ds_clip.rio.clip(cur_lau)\n",
    "\n",
    "            # Compute flood statistics for each floop_maps\n",
    "            clip_stats.compute(ds_clip)\n",
    "            \n",
    "            # Add stats to dataframe\n",
    "            lau_data['n_nans'][i] = clip_stats.n_nans\n",
    "            lau_data['n_less05'][i] = clip_stats.n_less05\n",
    "            lau_data['n_more05'][i] = clip_stats.n_more05\n",
    "            lau_data['total'][i] = clip_stats.total\n",
    "            lau_data['flooded'][i] = clip_stats.flooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce data frame to only those lau's that have been altered\n",
    "altered_lau_data = lau_data.loc[~lau_data.flooded.isna()]\n",
    "altered_lau_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "chunk_geom_df.plot(ax=ax, color='lightgray', edgecolor='black')\n",
    "lau_data.plot(ax=ax, color='gray')\n",
    "cmap = 'coolwarm'\n",
    "altered_lau_data.plot(ax=ax, column='flooded', cmap=cmap, legend=True)\n",
    "\n",
    "# Set the zoom level to the extent of the first plot\n",
    "ax.set_xlim(chunk_geom_df.total_bounds[0]-0.1e6, chunk_geom_df.total_bounds[2]+0.1e6)\n",
    "ax.set_ylim(chunk_geom_df.total_bounds[1]-0.1e6, chunk_geom_df.total_bounds[3]+0.1e6)\n",
    "\n",
    "ax.legend(['test1','test2','test4'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_assets(collection : pystac.Collection, exclude_asset_id : str ) -> int:\n",
    "\n",
    "    total_assets = 0\n",
    "\n",
    "    # Loop over each item in the collection\n",
    "    for item in collection.get_all_items():\n",
    "        # Loop over each asset in the item\n",
    "        for asset_key, asset in item.assets.items():\n",
    "            # Exclude assets id\n",
    "            if asset_key != exclude_asset_id:\n",
    "                total_assets += 1\n",
    "    return total_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe with all items and their bounding boxes\n",
    "\n",
    "def make_gdf_from_chunks(collection : pystac.Collection) -> gpd.GeoDataFrame:\n",
    "    \n",
    "    n_assets = find_n_assets(collection, exclude_asset_id='visual')\n",
    "\n",
    "    df = gpd.GeoDataFrame(columns = ['id','href','geometry'],index=range(n_assets))\n",
    "\n",
    "    i = 0 \n",
    "\n",
    "    for item in collection.get_all_items():\n",
    "\n",
    "        for chunk_title, chunk in item.get_assets().items():\n",
    "\n",
    "            if 'visual' in chunk_title:\n",
    "                continue\n",
    "\n",
    "            # Retrieve bounding box\n",
    "            # Retreive item geometry\n",
    "            [chunk_geom] = chunk.extra_fields.get('proj:geometry')['coordinates']\n",
    "\n",
    "            df['href'][i] = chunk.href                         # add href (URL to Google Bucket chunked COG)\n",
    "            df['id'][i] = pathlib.Path(chunk_title).as_posix() # add item id\n",
    "            df['geometry'][i] = shapely.Polygon(chunk_geom)    # add geometry as polygon\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tdf = make_gdf_from_chunks(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# New method is working and verified againts the old method, change within \n",
    "\n",
    "# Trial reduced LAU dataframe\n",
    "lau_data = gpd.read_parquet(r'p:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\LAU_stats\\LAU_2020_NUTS_2021_01M_3035_CM.parquet')\n",
    "\n",
    "# # For testing only do one coutnry\n",
    "# lau_data = lau_data.loc[lau_data.CNTR_CODE=='PT']\n",
    "# lau_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Initialize flood stats class\n",
    "clip_stats = flood_stats()\n",
    "\n",
    "tifs_df = make_gdf_from_chunks(collection)\n",
    "\n",
    "# for i, item in enumerate(collection.get_all_items()):\n",
    "for map_type in map_types:\n",
    "    for scen in path_list:\n",
    "\n",
    "        scen = pathlib.Path(scen).as_posix()\n",
    "        # Create a regex pattern that checks for both map_type and scen in any order\n",
    "        pattern = rf\"(?=.*{map_type})(?=.*{scen})\"\n",
    "\n",
    "        # Filter the DataFrame to only include rows where both map_type and scen are in the 'id' column\n",
    "        tif_df = tifs_df[tifs_df['id'].str.contains(pattern, regex=True)]\n",
    "\n",
    "        # Find all LAU's within a chunked tif based on the spatial join\n",
    "        lau_tif_join = lau_data.sjoin(tif_df, how = 'left', op = 'intersects')\n",
    "\n",
    "        # Sort based on the chunks\n",
    "        lau_tif_join = lau_tif_join.sort_values('href')\n",
    "\n",
    "        # Get unique chunks from joined dataframe do not consider NaN's\n",
    "        chunks = lau_tif_join.href.dropna().unique()\n",
    "        ids = lau_tif_join.id.dropna().unique()\n",
    "\n",
    "        # Iterate over chunks\n",
    "        for chunk, id in zip(chunks, ids):\n",
    "            \n",
    "            print('Now working on: ' + str(id))\n",
    "            # Load raw band_data dataset \n",
    "            ds = rio.open_rasterio(chunk, masked = True)\n",
    "\n",
    "            # Check if flooded pixels exist within the chunk\n",
    "            # if ds.isnull().all():\n",
    "            #     continue\n",
    "\n",
    "            # Select only LAU's that match with chunk\n",
    "            cur_chunk_lau = lau_tif_join[lau_tif_join.href.str.contains(chunk,na=False)]\n",
    "\n",
    "            # Initiate new columns in dataframe\n",
    "            col_name = Path(id).parent\n",
    "            if not str(col_name.joinpath('total')) in lau_data:\n",
    "            \n",
    "                lau_data[str(col_name.joinpath('n_nans'))] = None\n",
    "                lau_data[str(col_name.joinpath('n_less05'))] = None\n",
    "                lau_data[str(col_name.joinpath('n_more05'))] = None\n",
    "                lau_data[str(col_name.joinpath('total'))] = None\n",
    "            \n",
    "            # Iterate over LAU's \n",
    "            for i, cur_lau in cur_chunk_lau.iterrows():\n",
    "\n",
    "                t0 = time.time()\n",
    "\n",
    "                # First clip to bounding box\n",
    "                try:\n",
    "                    ds_clip = ds.rio.clip_box(*cur_lau.geometry.bounds)\n",
    "                except:\n",
    "                    print(\"Skipping LAU: Clipping resulted in a one-dimensional raster\")\n",
    "                    print(i)\n",
    "                    print(cur_lau.LAU_NAME)\n",
    "                    continue\n",
    "\n",
    "                # Check if flooded pixels exist within the bounding box\n",
    "                if ds_clip.isnull().all():\n",
    "                    continue\n",
    "\n",
    "                # Then, clip dataset to match AOI polygon\n",
    "                try:\n",
    "                    ds_clip = ds_clip.rio.clip([cur_lau.geometry])\n",
    "                except:\n",
    "                    print(\"Skipping LAU, because clipping to polygon does not work\")\n",
    "                    continue\n",
    "\n",
    "                # Check if flooded pixels exist within polygon\n",
    "                if ds_clip.isnull().all():\n",
    "                    continue\n",
    "\n",
    "                # Compute flood statistics for each floop_maps\n",
    "                clip_stats.compute(ds_clip)\n",
    "\n",
    "                # Update user\n",
    "                print(\"{:.2%}\".format(clip_stats.flooded) + \" flooded : \" + cur_lau.LAU_NAME)\n",
    "                \n",
    "                # Add stats to dataframe\n",
    "                if lau_data[str(col_name.joinpath('total'))][i] == None:\n",
    "                    lau_data[str(col_name.joinpath('n_nans'))][i] = clip_stats.n_nans\n",
    "                    lau_data[str(col_name.joinpath('n_less05'))][i] = clip_stats.n_less05\n",
    "                    lau_data[str(col_name.joinpath('n_more05'))][i]  = clip_stats.n_more05\n",
    "                    lau_data[str(col_name.joinpath('total'))][i]  = clip_stats.total\n",
    "                \n",
    "                else:\n",
    "                    print('Edge case detected, new chunk data is summed')\n",
    "\n",
    "                    lau_data[str(col_name.joinpath('n_nans'))][i] = lau_data[str(col_name.joinpath('n_nans'))][i] + clip_stats.n_nans\n",
    "                    lau_data[str(col_name.joinpath('n_less05'))][i] = lau_data[str(col_name.joinpath('n_less05'))][i] + clip_stats.n_less05\n",
    "                    lau_data[str(col_name.joinpath('n_more05'))][i]  = lau_data[str(col_name.joinpath('n_more05'))][i] + clip_stats.n_more05\n",
    "                    lau_data[str(col_name.joinpath('total'))][i]  = lau_data[str(col_name.joinpath('total'))][i] + clip_stats.total\n",
    "\n",
    "                print(time.time() - t0)\n",
    "\n",
    "        # Compute percentage flooded pixels per LAU, def: perc_flooded = 1-n_nans/n_total\n",
    "        lau_data[str(col_name.joinpath('flooded'))] = 1-lau_data[str(col_name.joinpath('n_nans'))]/lau_data[str(col_name.joinpath('total'))]\n",
    "\n",
    "LAU_NUTS_match = pd.read_csv(r'p:\\11207608-coclico\\FASTTRACK_DATA\\XX_NUTS\\lau_2020_nuts_2021_concordance_by_geo.csv')\n",
    "# Drop unneeded columns\n",
    "LAU_NUTS_match = LAU_NUTS_match.drop(columns=['gisco_id','country','lau_id','lau_name','population','area_km2','year'])\n",
    "LAU_NUTS_match = LAU_NUTS_match.rename(columns={'fid': 'FID'})\n",
    "\n",
    "# Merge the two dataframes\n",
    "LAU_NUTS_data = lau_data.merge(LAU_NUTS_match,on='FID')\n",
    "\n",
    "# Reorder columns\n",
    "# Retrieve column names\n",
    "cols = LAU_NUTS_data.columns\n",
    "\n",
    "# Move columns\n",
    "cols = cols.insert([2],cols[-2:])\n",
    "\n",
    "# Drop the old ones\n",
    "cols = cols[:-2]\n",
    "\n",
    "# Store in final form\n",
    "LAU_NUTS_data = LAU_NUTS_data[cols]\n",
    "\n",
    "# Write geodataframe\n",
    "coclico_data_dir = p_drive.joinpath(\"11207608-coclico\", \"FULLTRACK_DATA\")\n",
    "ds_dir = coclico_data_dir.joinpath('WP4','LAU_stats')\n",
    "out_file= ds_dir.joinpath('LAU_NUTS_CFHP_latest.parquet')\n",
    "\n",
    "# LAU_NUTS_data.to_parquet(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lau_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New method is working and verified againts the old method, change within \n",
    "\n",
    "# Trial reduced LAU dataframe\n",
    "lau_data = gpd.read_parquet(r'p:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\LAU_stats\\LAU_2020_NUTS_2021_01M_3035_CM.parquet')\n",
    "\n",
    "# # For testing only do one coutnry\n",
    "# lau_data = lau_data.loc[lau_data.CNTR_CODE=='PT']\n",
    "# lau_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Initialize flood stats class\n",
    "clip_stats = flood_stats()\n",
    "\n",
    "tifs_df = make_gdf_from_chunks(collection)\n",
    "\n",
    "map_types = [\"HIGH_DEFENDED_MAPS\", \"LOW_DEFENDED_MAPS\", \"UNDEFENDED_MAPS\"]\n",
    "rps = [\"static\", \"1\", \"100\", \"1000\"]  # 4 options\n",
    "scenarios = [\"None\", \"SSP126\", \"SSP245\", \"SSP585\", \"High_End\"]  # 5 options\n",
    "times = [\"2010\", \"2030\", \"2050\", \"2100\", \"2150\"]  # 5 options\n",
    "\n",
    "# for i, item in enumerate(collection.get_all_items()):\n",
    "\n",
    "for map_type in map_types:\n",
    "    for rp in rps:\n",
    "        for scen in scenarios:\n",
    "            for t in times:\n",
    "\n",
    "                # Create a regex pattern that checks for both map_type and scen in any order\n",
    "                pattern = rf\"(?=.*\\b{map_type}\\b)(?=.*\\b{rp}\\b)(?=.*\\b{scen}\\b)(?=.*\\b{t}\\b)\"\n",
    "\n",
    "                # Filter the DataFrame to only include rows where both map_type and scen are in the 'id' column\n",
    "                tif_df = tifs_df[tifs_df['id'].str.contains(pattern, regex=True)]\n",
    "\n",
    "                # Find all LAU's within a chunked tif based on the spatial join\n",
    "                lau_tif_join = lau_data.sjoin(tif_df, how = 'left', op = 'intersects')\n",
    "\n",
    "                # Sort based on the chunks\n",
    "                lau_tif_join = lau_tif_join.sort_values('href')\n",
    "\n",
    "                # Get unique chunks from joined dataframe do not consider NaN's\n",
    "                chunks = lau_tif_join.href.dropna().unique()\n",
    "                ids = lau_tif_join.id.dropna().unique()\n",
    "\n",
    "                # Iterate over chunks\n",
    "                for chunk, id in zip(chunks, ids):\n",
    "                    \n",
    "                    print('Now working on: ' + str(id))\n",
    "\n",
    "                    # Load raw band_data dataset \n",
    "                    ds = rio.open_rasterio(chunk, masked = True)\n",
    "\n",
    "                    # Check if flooded pixels exist within the chunk\n",
    "                    # if ds.isnull().all():\n",
    "                    #     continue\n",
    "\n",
    "                    # Select only LAU's that match with chunk\n",
    "                    cur_chunk_lau = lau_tif_join[lau_tif_join.href.str.contains(chunk,na=False)]\n",
    "\n",
    "                    # Initiate new columns in dataframe\n",
    "                    col_name = Path(id).parent\n",
    "                    if not str(col_name.joinpath('total')) in lau_data:\n",
    "                    \n",
    "                        lau_data[str(col_name.joinpath('n_nans'))] = None\n",
    "                        lau_data[str(col_name.joinpath('n_less05'))] = None\n",
    "                        lau_data[str(col_name.joinpath('n_more05'))] = None\n",
    "                        lau_data[str(col_name.joinpath('total'))] = None\n",
    "                    \n",
    "                    # Iterate over LAU's \n",
    "                    for i, cur_lau in cur_chunk_lau.iterrows():\n",
    "\n",
    "                        # First clip to bounding box\n",
    "                        try:\n",
    "                            ds_clip = ds.rio.clip_box(*cur_lau.geometry.bounds)\n",
    "                        except:\n",
    "                            print(\"Skipping LAU: Clipping resulted in a one-dimensional raster\")\n",
    "                            print(i)\n",
    "                            print(cur_lau.LAU_NAME)\n",
    "                            continue\n",
    "\n",
    "                        # Check if flooded pixels exist within the bounding box\n",
    "                        if ds_clip.isnull().all():\n",
    "                            continue\n",
    "\n",
    "                        # Then, clip dataset to match AOI polygon\n",
    "                        try:\n",
    "                            ds_clip = ds_clip.rio.clip([cur_lau.geometry])\n",
    "                        except:\n",
    "                            print(\"Skipping LAU, because clipping to polygon does not work\")\n",
    "                            continue\n",
    "\n",
    "                        # Check if flooded pixels exist within polygon\n",
    "                        if ds_clip.isnull().all():\n",
    "                            continue\n",
    "\n",
    "                        # Compute flood statistics for each floop_maps\n",
    "                        clip_stats.compute(ds_clip)\n",
    "\n",
    "                        # Update user\n",
    "                        print(\"{:.2%}\".format(clip_stats.flooded) + \" flooded : \" + cur_lau.LAU_NAME)\n",
    "                        \n",
    "                        # Add stats to dataframe\n",
    "                        if lau_data[str(col_name.joinpath('total'))][i] == None:\n",
    "                            lau_data[str(col_name.joinpath('n_nans'))][i] = clip_stats.n_nans\n",
    "                            lau_data[str(col_name.joinpath('n_less05'))][i] = clip_stats.n_less05\n",
    "                            lau_data[str(col_name.joinpath('n_more05'))][i]  = clip_stats.n_more05\n",
    "                            lau_data[str(col_name.joinpath('total'))][i]  = clip_stats.total\n",
    "                        \n",
    "                        else:\n",
    "                            print('Edge case detected, new chunk data is summed')\n",
    "\n",
    "                            lau_data[str(col_name.joinpath('n_nans'))][i] = lau_data[str(col_name.joinpath('n_nans'))][i] + clip_stats.n_nans\n",
    "                            lau_data[str(col_name.joinpath('n_less05'))][i] = lau_data[str(col_name.joinpath('n_less05'))][i] + clip_stats.n_less05\n",
    "                            lau_data[str(col_name.joinpath('n_more05'))][i]  = lau_data[str(col_name.joinpath('n_more05'))][i] + clip_stats.n_more05\n",
    "                            lau_data[str(col_name.joinpath('total'))][i]  = lau_data[str(col_name.joinpath('total'))][i] + clip_stats.total\n",
    "\n",
    "                # Compute percentage flooded pixels per LAU, def: perc_flooded = 1-n_nans/n_total\n",
    "                lau_data[str(col_name.joinpath('flooded'))] = 1-lau_data[str(col_name.joinpath('n_nans'))]/lau_data[str(col_name.joinpath('total'))]\n",
    "\n",
    "# Write geodataframe\n",
    "coclico_data_dir = p_drive.joinpath(\"11207608-coclico\", \"FULLTRACK_DATA\")\n",
    "ds_dir = coclico_data_dir.joinpath('WP4','LAU_stats')\n",
    "out_file= ds_dir.joinpath('LAU_NUTS_CFHP_all.parquet')\n",
    "\n",
    "lau_data.to_parquet(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tif_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial reduced LAU dataframe\n",
    "lau_data = gpd.read_parquet(r'p:\\11207608-coclico\\FULLTRACK_DATA\\WP4\\LAU_stats\\LAU_NUTS_CFHP_all.parquet')\n",
    "lau_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lau_tif_join.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file= ds_dir.joinpath('LAU_NUTS_CM_CFHP.parquet')\n",
    "lau_data.to_parquet(out_file)\n",
    "ds_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match LAU and NUTS using this work: https://edjnet.github.io/lau_centres/lau_nuts.html\n",
    "\n",
    "LAU_NUTS_match = pd.read_csv(r'p:\\11207608-coclico\\FASTTRACK_DATA\\XX_NUTS\\lau_2020_nuts_2021_concordance_by_geo.csv')\n",
    "# Drop unneeded columns\n",
    "LAU_NUTS_match = LAU_NUTS_match.drop(columns=['gisco_id','country','lau_id','lau_name','population','area_km2','year'])\n",
    "LAU_NUTS_match = LAU_NUTS_match.rename(columns={'fid': 'FID'})\n",
    "\n",
    "# Merge the two dataframes\n",
    "LAU_NUTS_data = test.merge(LAU_NUTS_match,on='FID')\n",
    "\n",
    "# Reorder columns\n",
    "# Retrieve column names\n",
    "cols = LAU_NUTS_data.columns\n",
    "\n",
    "# Move columns\n",
    "cols = cols.insert([2],cols[-2:])\n",
    "\n",
    "# Drop the old ones\n",
    "cols = cols[:-2]\n",
    "\n",
    "# Store in final form\n",
    "LAU_NUTS_data = LAU_NUTS_data[cols]\n",
    "\n",
    "# Write geodataframe to parquet\n",
    "coclico_data_dir = p_drive.joinpath(\"11207608-coclico\", \"FULLTRACK_DATA\")\n",
    "ds_dir = coclico_data_dir.joinpath('WP4','LAU_stats')\n",
    "out_file= ds_dir.joinpath('LAU_NUTS_CFHP.parquet')\n",
    "\n",
    "# altered_lau_data.to_parquet(out_file_altered)\n",
    "LAU_NUTS_data.to_parquet(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAU_NUTS_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coclico",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
